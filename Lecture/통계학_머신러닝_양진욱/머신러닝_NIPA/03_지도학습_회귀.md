# 3. 지도학습_회귀

## 1) 회귀 개념 알아보기

- 데이터를 가장 잘 설명하는 모델을 찾아 입력값에 따른 미래 결과값을 예측하는 알고리즘
- 독립변수 X와 종속변수 Y에 대해 이 데이터 간의 관계를 가장 잘 설명할 수 있는 관계식을 찾아내는 것
- 완벽한 예측은 불가능하기 때문에, 가능한 실제 값과 모델이 예측하는 값의 차이를 최소화할 수 있는 관계식을 찾아야함
- 예 : 독립변수 X=과거 평균 기온, 종속변수 Y=아이스크림 판매량이라고 할때, 현재 평균 기온에 따른 아이스크림 판매량을 예측할 수 있는 관계식을 찾는 것

## 2) 단순 선형 회귀

- 단순 선형 회귀란?
    - 데이터를 설명하는 모델을 직선 형태($\hat Y=\beta_0+\beta_1 \hat X$)로 가정하고, 이를 구성하는 $\beta_0$(y절편), $\beta_1$(기울기)를 구하는 것

- 단순 선형 회귀의 특징
    - 가장 기초적이나 여전히 많이 사용되는 알고리즘
    - 입력값이 1개인 경우에만 적용 가능
    - 입력값과 결과값의 관계나 영향을 파악하는 데 용이
    - 두 변수 간의 관계를 직관적으로 해석하고자 하는 경우에 활용함
    
- loss 함수
    - 실제값과 예측값의 차이를 제곱하여 모두 더하여 총 개수로 나눈 값
    - ${1 \over N} \times {\sum_{i}^N(Y_i-\hat Y_i)^2}$ = ${1 \over N} \times {\sum_{i}^N(Y_i-(\beta_0+\beta_1 \hat X_i))^2}$
    - loss값의 최소화를 통해 예측값이 실제값과 최대한 근사하도록 해야함
    - 입력값과 실제값은 주어져있으므로, $\beta_0$와 $\beta_1$을 조절하여 최소화하며 사용하는 방법은 아래와 같음
        - Gradient descent(경사하강법)
            - 한 번의 계산을 통해 loss함수를 최소화하는 베타값을 찾는 것이 아니라 초기값에서 점진적으로 구하는 방식
            - 과정
                1. $\beta_0$와 $\beta_1$를 랜덤하게 초기화
                2. loss 값 계산
                3. 현재 loss 값을 줄일 수 있는 새로운 $\beta_0$와 $\beta_1$를 찾기 위해 gradient 계산
                4. 계산한 gradient를 활용해 $\beta_0$와 $\beta_1$ 갱신
                5. loss 값의 차이가 거의 없어질 때까지 2~4번 과정 반복
        - Normal equation(least squares)
        - Brute force search
        - 그 외 방법도 존재

## 3) 다중 선형 회귀

- 다중 선형 회귀란?
    - 단순 선형 회귀의 입력값이 1개라면 다중 선형 회귀의 입력값은 2개 이상
    - 즉, 여러 개의 입력값으로 결과값을 예측하고자 하는 경우에 사용하는 알고리즘
    - $\hat Y=\beta_0+\beta_1 \hat X_1+\beta_2 \hat X_2+\beta_3 \hat X_3+...++\beta_M \hat X_M$
    
- 다중 선형 회귀의 특징
    - 여러 개의 입력값과 결과값 간의 관계 파악
    - 각 입력값의 결과값에 대한 영향력을 알 수 있음
    - 입력값 사이의 상관 관계가 높을 경우, 결과에 대한 신뢰성을 잃을 가능성이 존재

- loss 함수
    - 단순 선형 회귀와 개념적으로 동일
    - 다만, 베타값이 입력값의 개수에 따라 2개 이상으로 존재할 수 있음

## 4) 회귀 평가 지표

- 회귀 평가 지표란?
    - 특정 모델의 예측값이 실제값과 얼마나 차이가 있는지에 기반해 모델의 성능을 평가할 수 있도록 한 지표
    
- 회귀 평가 지표의 종류
    - RSS(Residual Sum of Squares)
        - 실제값과 예측값의 차이를 제곱하여 더한 값
        - $RSS={\sum_{i}^N(Y_i-(\beta_0+\beta_1 \hat X_i))^2}$
        - 값이 작을수록 모델의 성능이 좋음
        - 가장 간단한 평가 방법으로 직관적인 해석이 가능
        - 오차를 그대로 이용하기 때문에 입력 값의 크기에 의존적
        - 절대적인 값과 비교가 불가
        
    - MSE(Mean Squared Error)
        - RSS를 데이터의 개수로 나눈 값
        - $MSE={1 \over N} \times {\sum_{i}^N(Y_i-(\beta_0+\beta_1 \hat X_i))^2}$
        - 값이 작을수록 모델의 성능이 좋음
        - 이상치에 민감
        
    - MAE(Mean Absolute Error)
        - 실제값과 예측값 차이의 절대값을 모두 더한 뒤 데이터의 개수로 나눈 값
        - $MAE={1 \over N} \times {\sum_{i}^N|Y_i-(\beta_0+\beta_1 \hat X_i)|}$
        - 값이 작을수록 모델의 성능이 좋음
        - 변동성이 큰 지표와 낮은 지표를 같이 예측할 시 유용
        
    - 결정계수($R^2$)
        - 회귀 모델의 설명력을 표현하는 지표
        - $R^2=1-{RSS \over TSS}$
        - $RSS={\sum_{i}^N(Y_i-(\beta_0+\beta_1 \hat X_i))^2}$
        - $TSS={\sum_{i}^N(Y_i-\bar Y_i)^2}, \quad \bar Y_i=평균$
        - 1에 가까울수록 높은 성능의 모델, 즉 오차가 적다고 해석 가능
        - 값이 0인 경우, 데이터의 평균 값을 출력하는 직선 모델을 의미
        - 음수값이 나온 경우, 평균값 예측보다 성능이 좋지 않음을 의미