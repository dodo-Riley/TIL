# 3.2.2. 기초 통계분석

## 1) 기술통계

- 기술통계(descriptive statistics)
    - 자료를 요약하는 기초적 통계
    - 분석에 앞서 데이터의 대략적인 통계적 수치를 계산해봄으로써 데이터에 대한 대략적인 이해와 분석에 대한 통찰력을 얻기에 유리함
    
    ```r
    > data(iris) # 예제 데이터 불러오기
    
    > head(iris) # 데이터의 기본 6행을 보여줌
      Sepal.Length Sepal.Width Petal.Length Petal.Width Species
    1          5.1         3.5          1.4         0.2  setosa
    2          4.9         3.0          1.4         0.2  setosa
    3          4.7         3.2          1.3         0.2  setosa
    4          4.6         3.1          1.5         0.2  setosa
    5          5.0         3.6          1.4         0.2  setosa
    6          5.4         3.9          1.7         0.4  setosa
    
    > head(iris,3) # 데이터의 기본 3행을 보여줌
      Sepal.Length Sepal.Width Petal.Length Petal.Width Species
    1          5.1         3.5          1.4         0.2  setosa
    2          4.9         3.0          1.4         0.2  setosa
    3          4.7         3.2          1.3         0.2  setosa
    
    > tail(iris) # 데이터의 마지막 6행을 보여줌
        Sepal.Length Sepal.Width Petal.Length Petal.Width   Species
    145          6.7         3.3          5.7         2.5 virginica
    146          6.7         3.0          5.2         2.3 virginica
    147          6.3         2.5          5.0         1.9 virginica
    148          6.5         3.0          5.2         2.0 virginica
    149          6.2         3.4          5.4         2.3 virginica
    150          5.9         3.0          5.1         1.8 virginica
    
    > head(iris,3); tail(iris,3) # 데이터의 처음 3행, 마지막 3행을 보여줌
      Sepal.Length Sepal.Width Petal.Length Petal.Width Species
    1          5.1         3.5          1.4         0.2  setosa
    2          4.9         3.0          1.4         0.2  setosa
    3          4.7         3.2          1.3         0.2  setosa
        Sepal.Length Sepal.Width Petal.Length Petal.Width   Species
    148          6.5         3.0          5.2         2.0 virginica
    149          6.2         3.4          5.4         2.3 virginica
    150          5.9         3.0          5.1         1.8 virginica
    
    > summary(iris) # 데이터의 컬럼에 대한 전반적인 기초 통계량을 보여줌줌
      Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
     Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
     1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
     Median :5.800   Median :3.000   Median :4.350   Median :1.300  
     Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
     3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
     Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
           Species  
     setosa    :50  
     versicolor:50  
     virginica :50
    
    > summary(iris$Sepal.Width) # iris데이터의 Sepal.Width 컬럼의 기초 통계량을 보여줌줌
       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      2.000   2.800   3.000   3.057   3.300   4.400
    
    > mean(iris$Sepal.Length) # 평균
    [1] 5.843333
    > median(iris$Sepal.Length) # 중위수
    [1] 5.8
    > sd(iris$Sepal.Length) # 표준편자
    [1] 0.8280661
    > var(iris$Sepal.Length) # 분산
    [1] 0.6856935
    > max(iris$Sepal.Length) # 최대값
    [1] 7.9
    > min(iris$Sepal.Length) # 최소값
    [1] 4.3
    > quantile(iris$Sepal.Length,1/4) # 1사분위수
    25% 
    5.1 
    > quantile(iris$Sepal.Length,3/4) # 3사분위수
    75% 
    6.4
    ```
    

---

## 2) 회귀분석

- 회귀분석(regression analysis)
    - 독립변수가 종속변수에 미치는 영향력을 분석하거나, 독립변수에 따라 종속변수의 변화를 예측하기 위해서 사용하는 통계기법
        - 독립변수 : 종속변수에 영향을 주는 변수로 설명변수라고도 함
        - 종속변수 : 독립변수에 영향을 받는 변수로 반응변수라고도 함
    - 영국의 유전학자 Galton이 두 변수 간의 상관과 회귀에 관한 분석 방법을 처음 제시
    - 회귀라는 단어 자체의 뜻은 현상들이 평균으로 향하는 경향을 뜻함
    
- 목적
    - 종속변수와 독립변수들 사이에 존재하는 함수 관계를 추정
    - 독립변수들이 종속변수에 미치는 효과를 검정
    - 추정된회귀함수를 이용해 종속변수의 미래 값을 예측
    
- 상관분석과의 차이점
    - 상관분석은 둘 이상의 변수들이 어느 정도 상관성을 가지는지 분석하는 것이 주 목적
    - 회귀분석은 둘 이상의 변수들 간에 미치는 영향관계를 통한 예측이 목표
    
- 회귀분석을 사용하는 경우
    - 회귀분석은 변수들 중 하나를 종속변수로, 나머지를 독립변수로 하여 변수들 간에 상관관계가 존재할 때, 독립변수가 한 단위 변화함에 다라 종속변수가 어떻게 변화하는지를 분석하는 기법
    - 자료의 척도는 일반적으로 등간척도 또는 비율척도여야 함
    - 독립변수가 범주형 척도이면, 이를 가변수(더미변수)를 만들어 이용
    - 종속변수가 이변량 변수이면 로지스틱 회귀분석을 실시
    
- 최소제곱법(최소자승법)
    - 실제 데이터를 측정하다보면, 독립변수에 따라 종속변수의 변화하는 정도가 다르게 나타나는 경우처럼 개별 측정치들 간에 차이가 발생함
    - 표본으로부터 도출된 회귀식을 $\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X_i+\epsilon_i$, 미지의 모회귀식을 $Y=\beta_0+\beta_1X_i$일 때, 표본에서의 $\hat{\beta}$를 모수 $\beta$에 가장 가깝게 추정한 회귀식을 도출하는 것이 가장 이상적
    - $\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X_i+\epsilon_i$, 회귀식과 측정치 간의 차이인 잔차 $\epsilon_i$가 필연적으로 발생하며, 추정회귀모형은 $\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X_i+\epsilon_i$, 잔차의 모든 합이 최소가 되는 회귀식을 구함
    - $\hat{Y}=\hat{\beta_0}+\hat{\beta_1}X_i$의 직선상에 위(+)와 아래(-)로 분포되어 단순 합은 0이 되므로 이를 모두 제곱하여 합을 구함
    - 이러한 잔차 제곱의 합을 SSE(sum of squared error)라 함
    - 이와 같이 구해진 회귀계수의 추정량, $\hat{\beta_0},\hat{\beta_1}$을 최소제곱추정량(LSE, least squares estimator)라고 함
    
- 회귀분석의 종류
    - 독립변수의 수에 따라
        - 단순회귀분석
            - 종속변수에 있어서 하나의 독립변수에 대한 회귀식을 추정하는 방식
            - 단순회귀모형 : $y_i=\beta_0+\beta_1x_i+\epsilon_i, \quad i=1,2,3,...,n$
                - $y_i$ : i번째 종속변수의 값
                - $x_i$ : i번째 독립변수의 값
                - $\beta_0$ : 선형회귀식의 절편
                - $\beta_1$ : 선형회귀식의 기울기
                - $\epsilon_i$ : 오차항으로 $\epsilon_i$는 독립적이며 $N(0, \delta^2)$의 분포를 이룸
            - 각종 주요 통계량
                
                
                | 통계량 | 설명 |
                | --- | --- |
                | 잔차(residual) | 실제 관측된 ⁍와 추정된 ⁍의 차이, 즉 ⁍  |
                | SSE(sum of squared error) | - 잔차 제곱의 합
                - 회귀식으로 설명되지 않는 제곱합 |
                | ⁍ | ⁍의 평균 |
                | SST(sum of squared total) | - ⁍의 평균 대비 변동 차이의 제곱 합
                - ⁍
                - SSE와 SSR의 합과 같음 |
                | SSR(sum of squared regression) | - 추정된 데이터 ⁍와 평균의 차이 제곱합
                - ⁍
                - 회귀식으로 설명되는 제곱합 |
                | 결정계수(⁍) | - ⁍
                - ⁍ |
                | MSR, MSE | - SSR과 SSE를 각각 자유도로 나눈 값
                - 회귀모형의 자유도는 1이고 잔차의 자유도는 n-2이므로 아래와 같음
                - ⁍
                - ⁍ |
                | F 통계량 | - MSR/MSE |
        - 다중회귀분석
            - 종속변수에 있어서 둘 이상의 독립변수에 대한 회귀식을 추정하는 방식
            - 다중회귀모형 : $y_i=\beta_0+\beta_1x_{1,i}+\beta_2x_{2,i}+\epsilon_i, \quad i=1,2,3,...,n$
            - 주요 통계량은 단순회귀와 대부분 비슷한 꼴이나, 자유도에서 차이가 있음
                - 따라서 F 통계량이 $MSR/MSE={{SSR/k} \over {SSE/(n-k-1)}}$이 됨
    - 독립변수 척도에 따라
        - 일반회귀 : 등간, 비율 척도
        - 더미회귀 : 명목, 서열 척도
    - 독립/종속변수의 관계에 따라
        - 선형회귀
        - 비선형회귀

- 회귀분석 모형에서 체크해야 할 사항
    
    
    | 구분 | 확인사항 |
    | --- | --- |
    | 회귀모형이 통계적으로 유의미한가? | 유의수준 5%하에서 F 통계량의 p값이 0.05보다 작으면 추정된 회귀식은 통계적으로 유의하다고 볼 수 있음 |
    | 회귀계수들이 유의미한가? | 해당 계수의 t 통계량과 p값 또는 이들의 신뢰구간을 확인 |
    | 모형이 얼마나 설명력을 갖는가? | 높은 값을 가질수록 추정된 회귀식의 설명력이 높고, 0~1의 값을 가지는 결정계수를 확인 |
    | 모형이 데이터를 잘 적합하고 있는가? | 잔차의 그래프를 그리고 회귀진단을 함 |
    | 데이터가 모형 가정을 만족 시키는가? | 아래 회귀모형에 대한 가정 참고 |
    
- 회귀모형에 대한 가정
    - 선형성 : 독립변수의 변화에 따라 종속변수도 변화하는 선형(linear)모형
    - 독립성 : 잔차와 독립변수의 값이 관련되어 있지 않음
    - 등분산성 : 오차항들의 분포는 동일한 분산을 가짐
    - 비상관성 : 잔차들끼리 상관이 없어야 함
    - 정상성 : 잔차항이 정규분포를 이뤄야 함
    
- 회귀분석의 모형 가정을 확인하는 방법
    - 회귀모형에서 오차항에 대한 3가지 가정을 전제로 함
    - 정규성, 등분산성, 독립성에 대한 가정이 필요하며 이런 가정이 성립해야 회귀분석 결과가 타당한 것이 됨
    - 잔차를 오차항의 관찰값으로 해석할 수 있으므로, 잔차들을 분석해 봄으로써 오차항에 대한 가정들의 성립 여부를 확인
    - 이러한 분석을 잔차(residual)분석이라고 함
    
    | 선형성 | - 독립변수가 변화할 대 종속변수가 일정한 크기로 변화한다면 선형성을 만족하며, 산점도를 통해 확인
    - 잔차의 산점도가 곡선 형태를 보인다면 비선형성을 의미
    - 일러 경우 추가적으로 독립변수의 제곱항이 필요 |
    | --- | --- |
    | 등분산성 | - 잔차와 예측치 산점도가 부채꼴이면 등분산성이 무너지고 오차항이 이분산성을 갖는다고 함
    - 분산이 일정하지 않으면 가중회귀를 쓰거나 종속변수를 변화시킴
    - 이분산성은 독립변수 값이 변화할 대 종속변수 값들의 분산이 상이하게 될때 나타남 |
    | 독립성 | - 더빈-왓슨 테스트는 회귀분석 후 잔차의 독립성을 확인할 때 쓰이는 테스트
    - 잔차끼리 자기상관성이 있는지 없는지를 판단
    - 더빈 왓슨 계수는 0초과 4미만의 값을 가지며, 1.5초과 2.5미만이면 자동 상관이 없는 것으로 판단 |
    | 정규성 또는 정상성 | - Q-Q plot을 그려서 정규성 가정이 만족되는지 시작적으로 확인
    - Q-Q plot은 대각선 참조선을 따라서 값들이 분포하게 되면 정규성을 만족한다고 볼 수 있음
    - 만약 한족으로 치우치는 모습이라면 정규성 가정에 위배되었다고 볼 수 있음
    - Shapiro-Wilk test, Kolmogorov-Smirnov test, 앤더슨 달링 검정등이 정규성을 확인할 수 있는 방법 |
    
- 회귀분석 용어 정리
    - 귀무가설(영가설)
        - 실험이나 관찰을 통해서 기각하고 싶은 기존 가설
        - 측정된 값들은 회귀식으로 설명할 수 없음
        - 독립변수는 종속변수에 영향을 주지 않음
    - 대립가설(연구가설)
        - 실험이나 관찰을 통해서 주장하고 싶은 새로운 이론
        - 측정된 값들은 회귀식으로 설명할 수 있음
        - 독립변수는 종속변수에 영향을 줌
    - 기각
        - 통계적으로 유의하다
        - 대립가설이 참이다
    - 기각역
        - 확률분포에서 귀무가설을 기각하는 영역
        - 기각역에 검정통계량이 위치하면 귀무가설을 기각함
        - 대부분의 통계에서 p값(유의확률)이 0.05보다 작으면 귀무가설을 기각
        - 회귀모형에서는 기각 대신 유의하다고 함
    - 유의수준
        - 가설을 검정할 때 어느 정도까지 벗어나면 귀무가설이 오류라고 인정하는 수준을 말함
    
- 다중회귀분석 결과 해석 방법
    
    ```r
    > data("attitude")
    
    > tail(attitude)
       rating complaints privileges learning raises critical advance
    25     63         54         42       48     66       75      33
    26     66         77         66       63     88       76      72
    27     78         75         58       74     80       78      49
    28     48         57         44       45     51       83      38
    29     85         85         71       71     77       74      55
    30     82         82         39       59     64       78      39
    
    > out<-lm(rating~.,data=attitude)
    # out<-lm(rating~complaints+privileges+learning+raises+critical+advance,data=attitude)와 동일한 의미
    # 선형모형을 입력할 때 찍힌 .은 ‘그 외의 모든 변수’를 의미
    
    > summary(out)
    
    Call:
    lm(formula = rating ~ ., data = attitude)
    
    Residuals:
         Min       1Q   Median       3Q      Max 
    -10.9418  -4.3555   0.3158   5.5425  11.5990 
    
    Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
    (Intercept) 10.78708   11.58926   0.931 0.361634    
    complaints   0.61319    0.16098   3.809 0.000903 ***
    privileges  -0.07305    0.13572  -0.538 0.595594    
    learning     0.32033    0.16852   1.901 0.069925 .  
    raises       0.08173    0.22148   0.369 0.715480    
    critical     0.03838    0.14700   0.261 0.796334    
    advance     -0.21706    0.17821  -1.218 0.235577    
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
    
    Residual standard error: 7.068 on 23 degrees of freedom
    Multiple R-squared:  0.7326,	Adjusted R-squared:  0.6628 
    F-statistic:  10.5 on 6 and 23 DF,  p-value: 1.24e-05
    ```
    
    - Residuals
        - median을 가운데로 두고 다른 분위수가 대칭적으로 나타나는지 정도만 봐도 무방.
        - 분위수만으로는 제대로 된 모형진단을 할 수 없기 때문에 보나 마나 큰 의미가 없음
    - Estimate
        - 각 변수에 따른 비표준화 회귀계수로, (Intercept)는 회귀직선의 *y* 절편, 나머지는 변수마다의 단위변화율을 나타냄
        - 단순회귀분석과 달리 여러가지 독립변수가 있기 때문에 유의수준에 따라 결과가 달라짐.
        - 유의수준 5%에서는 learning의 회귀계수가 0이라는 귀무가설이 기각되지 못하므로 $(rating)=0.61319⋅(complaints)+ε$
        - 반면 유의수준 10 에서는 $(rating)=0.61319⋅(complaints)+0.32033⋅(learning)+ε$
        - 다만, r에서는 기본적으로 표준화된 회귀계수를 알려주지 않음
        - 따라서 QuantPsyc 패키지에 있는 lm.beta() 함수를 이용해서 확인
    - std. Error
        - 표준오차
    - t value
        - 절대값의 크기는 독립변수들 간에 종속변수에 영향력의 상대적 크기를 의미
    - pr(>|t|)
        - p값
        - 이게 작다는 것은 회귀계수가 유의하다는 뜻이고, 상관관계가 있다는 것을 보이고 싶다면 값이 작을수록 좋음.
        - 이 값이 얼마나 작은지에 따라 바로 아래에 있는 Signif. codes대로 점이 찍히게 됨.
        - 보통 유의수준은 5%로 잡으므로, 점 하나만 찍혀도 상관관계는 있다고 봐도 무방.
        - 만약 이 값이 커서 회귀관계가 없는 것으로 밝혀진다면 회귀계수가 어떻게 구해지든 통계적으로는 의미가 없음.
        - 이 p값은 t분포에서 나온 것이므로 pr(>|t|) 라는 표현이 적절함.
    - Adjusted R-squared
        - 수정된 결정계수($1-{{(n-1)(1-R^2)}\over{n-p-1}}, \quad n=표본의 크기,p=독립변수의수)$
        - 이 분석이 변수의 갯수에 비해 데이터를 얼마나 잘 설명하는가를 나타내는 척도로써, 높을수록 좋음.
        - 위 모형의 설명력은 약 66.82%
        - 일반적으로 수저된 결정계수를 사용하는 이유는 회귀모형에서 독립변수의 수가 많아지면 결정계수는 증가하게 되고, 이를 위해 독립변수의 수에 영향을 받지 않는 수정 결정 계수가 필요하기 때문
    - F-statistiic, p-value
        - p-value가 작다는 것은 유의한 회귀계수가 있다는 뜻이고, 상관관계가 있다는 것을 보이고 싶다면 값이 작을수록 좋음.
        - 회귀계수의 t검정이 회귀계수 하나하나에 대한 검정이라면, F검정은 회귀분석 자체에 대한 검정
        - 유의확률이 유의수준에서 작으므로 회귀모형은 통계적으로 타당

> 참고 : [https://freshrimpsushi.github.io/posts/how-to-interpret-multiple-regression-summary-in-r/](https://freshrimpsushi.github.io/posts/how-to-interpret-multiple-regression-summary-in-r/)
> 

- 단순회귀분석 결과 해석
    
    ```r
    > data("ChickWeight")
    > head(ChickWeight)
      weight Time Chick Diet
    1     42    0     1    1
    2     51    2     1    1
    3     59    4     1    1
    4     64    6     1    1
    5     76    8     1    1
    6     93   10     1    1
    
    > chick = ChickWeight[ChickWeight$Chick==1,] # Chick 컬럼의 값이 1인 데이터만 선택 
    > lm(weight~Time,data=chick)
    
    Call:
    lm(formula = weight ~ Time, data = chick)
    
    Coefficients:
    (Intercept)         Time  
         24.465        7.988
    
    > m1 = lm(weight~Time,data=chick) # 단순회귀분석(lm(종속변수~독립변수,데이터)
    > summary(m1)
    
    Call:
    lm(formula = weight ~ Time, data = chick)
    
    Residuals:
         Min       1Q   Median       3Q      Max 
    -14.3202 -11.3081  -0.3444  11.1162  17.5346 
    
    Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
    (Intercept)  24.4654     6.7279   3.636  0.00456 ** 
    Time          7.9879     0.5236  15.255 2.97e-08 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
    
    Residual standard error: 12.29 on 10 degrees of freedom
    Multiple R-squared:  0.9588,	Adjusted R-squared:  0.9547 
    F-statistic: 232.7 on 1 and 10 DF,  p-value: 2.974e-08
    ```
    
    - 분석 결과 $weight = 7.988Time+24.465$를 추정
    - F통계량은 232.7, p값은 2.974e-08, 결정계수들이 95%이상으로 회귀모형은 통계적으로 매우 유의함
    
- 잔차분석을 통한 회귀분석의 모형 가정
    
    ```r
    > data(cars)
    > m = lm(dist~speed, cars)
    > summary(m)
    
    Call:
    lm(formula = dist ~ speed, data = cars)
    
    Residuals:
        Min      1Q  Median      3Q     Max 
    -29.069  -9.525  -2.272   9.215  43.201 
    
    Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
    (Intercept) -17.5791     6.7584  -2.601   0.0123 *  
    speed         3.9324     0.4155   9.464 1.49e-12 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
    
    Residual standard error: 15.38 on 48 degrees of freedom
    Multiple R-squared:  0.6511,	Adjusted R-squared:  0.6438 
    F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12
    
    > par(mfrow=c(2,2)) # 화면 2*2로 그리기
    > plot(m)
    
    ```
    
    ![Untitled](3%202%202%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2086caee82180445e5b95b81ec3ce59d14/Untitled.png)
    
    - 왼쪽 상단
        - y축은 잔차, 0을 기준으로 분포가 좌우 균등하면 잔차들은 등분산성을 만족
    - 오른쪽 상단
        - 잔차들이 그래프선 상에 있으면 정상성을 만족
    - 왼쪽 하단
        - y축이 표준화 잔차, 0에서 멀리 떨어진 값이 있다면 이상치일 가능성이 높음
    - 오른쪽 하단
        - 이상치처럼 영향을 크게 주는 데이터 확인 가능
        
- 다항 회귀분석
    
    ```r
    x <- c(1,2,3,4,5,6,7,8,9)
    y <- c(5,3,2,3,4,6,10,12,18)
    df1 <- data.frame(x,y)
    m1<-lm(y~x,df1)
    par(mfrow=c(2,2))
    plot(m1)
    
    x2 = x^2
    df2 = cbind(x2,df1)
    plot(lm(y~x+x2,df2))
    ```
    
    ![Untitled](3%202%202%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2086caee82180445e5b95b81ec3ce59d14/Untitled%201.png)
    
    ![Untitled](3%202%202%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2086caee82180445e5b95b81ec3ce59d14/Untitled%202.png)
    
    - 첫 그래프와 같은 데이터 분포는 회귀식의 잔차도가 뚜렷한 곡선패턴을 가지기 때문에 구하고자 하는 모델식은 이차항이 포함된 모델이라고 가정할 수 있음
    - 이차항을 포함시켜 다시 계산해보면 기존보다 안정된 형태의 잔차를 보임
    - 결정계수와 통계적 모형 역시 summary()를 통해 확인하면 통계적으로 매우 유의함
    
- 다중공선성
    - 모형의 일부 독립변수가 다른 독립변수와 상관되어 있을 때 발생
    - 회귀계수의 분산을 증가시켜 불안정하고 해석하기 어렵게 만들기 떄문에 문제가 됨
    - 분산 팽창계수 $VIF={1 \over 1-R^2}$ 로 여부를 판정하며 10이하이면 문제가 적은 것으로 판단
    - 결정계수 값은 높으나 독립변수의 유의확률 값이 커서 개별 독립변수들이 유의하지 않는 경우, 다중공선성을 의심해봐야 함
    - 해결방안
        - 중요하지 않은 변수 제거
        - 능형회귀, 주성분회귀 등 편의 추정법을 사용
        - 자료부족이 원인일 경우 자료 보완
        
- 회귀분석모형의 적합도검정과 분산분석표
    - 회귀분석의 분산분석표 목적
        - 모형의 적합성 제시
            - 적합성 검정 : 도출한 회귀식이 표본 측정치를 얼마나 잘 설명하는지를 확인하는 것
        - 표본에 대한 회귀선의 설명력
            - 추정된 회귀식이 어느 정도 측정치들과 일치하는지의 정도
            - 설명력은 0~1까지의 숫자로 나타내거나 몇 %의 설명력을 가지는지 확률로 표현함
            - 이는 결정계수 $R^2$로 판단
        - 분산분석
            - 회귀분석에서 분산분석이 등장하는 이유는 총편차를 분해하는 과정이 분산분석과 동일하기 때문
            - 분산비율 F값은 단순회귀 모형 주요 통계량에서 언급한 F통계량임
            - 회귀식의 설명력에 대한 검정을 한 후 표본을 설명하는 회귀식이 유의한지를 확인하는 과정이 필요
            - 회귀잔차에 대한 분산비율을 F값을 계산해서 F분포표의 값보다 더 크면 귀무가설을 기각하고 대립가설을 채택
            - 이때, 귀무가설은 회귀식이 유의하다이며 대립가설은 유의하지 않다임
            - 회귀모형의 변동은 총변동(SST), 회귀변동(SSR), 오차변동(SSE) 3가지로 구분되며 이 3가지 제곱합을 자유도로 각각 나누면 일종의 분산이 됨
            - 이에 따라 분산분석표를 만들면 아래와 같음
                
                
                | 변동 | 제곱합 | 자유도 | 평균제곱 | F |
                | --- | --- | --- | --- | --- |
                | 회귀 | SSR | 1 |  ⁍ | MSR/MSE |
                | 오차 | SSE | n-2 | ⁍ |  |
                | 전체 | SST | n-1 |  |  |
                
- 가변수(더미변수)를 이용한 회귀분석
    - 가변수를 이용한 회귀분석은 명목척도나 범주형 척도의 자료를 독립변수의 가변수로 변환하여 회귀분석하는 것을 의미
    - 가변수란 명목변수의 측정치의 존재여부에 따라 0과 1의 값을 이용해 변수를 변환시킨 것
        - 따라서 명목변수의 범주 수에서 1을 뺀것과 같음
        - 예 : 남자와 여자 → 남자면 0 여자면 1 → 1개의 가변수가 존재함
        
- 최적회귀방정식의 선택(독립변수의 선택)
    - 종속변수에 영향을 미치는 독립변수는 예측에 모두 참여시키되, 가능한 범위 내에서 최소화 시켜야 함
    - 단계적 변수 선택
        - 전진선택법(forward selection)
            - 절편만 있는 상수 모형으로부터 시작해 중요하다고 생각되는 독립변수부터 차례로 모형에 추가
            - 후보가 되는 독립변수 중 추가했을 때 제곱합의 기준으로 가장 설명을 잘하는 변수를 고려하여 그 변수가 유의하면 추가하고 그렇지 않은 경우 추가 중단
            - 한번 추가된 변수는 제거할 수 없음
        - 후진제거법(backward elimaination)
            - 후보가 되는 독립변수를 모두 포함하는 모형에서 출발하여 제곱합의 기준으로 가장 적은 영향을 주는 변수부터 하나씩 제거하면서 더 이상 유의하지 않는 변수가 없을 때까지 독립변수를 제거
            - 한번 제거된 변수는 추가할 수 없음
        - 단계별방법 (stepwise method)
            - 전진선택법에 의해 변수를 추가하면서 새롭게 추가된 변수에 기인해 기존 변수의 중요도가 약화되면 해당 변수를 제거하는 등 단계별로 추가 또는 제거되는 변수의 여부를 검토해 더 이상 없을 때 중단
        - step() 함수를 이용한 단계적 변수 선택
        
        ```r
        > x1 = c(7,1,11,11,7,11,3,1,2,21,1,11,10)
        > x2 = c(26,29,56,31,52,55,71,31,54,47,40,66,68)
        > x3 = c(6,15,8,8,6,9,17,22,18,4,23,9,8)
        > x4 = c(60,52,20,47,33,22,6,44,22,26,34,12,12)
        > y = c(78.5,74.3,104.3,87.6,95.9,109.2,102.7,72.5,93.1,115.9,83.8,113.3,109.4)
        > df = data.frame(x1,x2,x3,x4,y)
        > step(lm(y~1,df), scope=list(lower=~1,upper=~x1+x2+x3+x4), direction='forward')
        # step(lm(종속변수~독립변수,데이터셋), scope=list(lower=~1,upper=~독립변수),direction='변수선택방법')
        # forward(전진선택), backward(후진제거), both(단계별방법)
        Start:  AIC=71.44
        y ~ 1
        
               Df Sum of Sq     RSS    AIC
        + x4    1   1831.90  883.87 58.852
        + x2    1   1809.43  906.34 59.178
        + x1    1   1450.08 1265.69 63.519
        + x3    1    776.36 1939.40 69.067
        <none>              2715.76 71.444
        
        Step:  AIC=58.85
        y ~ x4
        
               Df Sum of Sq    RSS    AIC
        + x1    1    809.10  74.76 28.742
        + x3    1    708.13 175.74 39.853
        <none>              883.87 58.852
        + x2    1     14.99 868.88 60.629
        
        Step:  AIC=28.74
        y ~ x4 + x1
        
               Df Sum of Sq    RSS    AIC
        + x2    1    26.789 47.973 24.974
        + x3    1    23.926 50.836 25.728
        <none>              74.762 28.742
        
        Step:  AIC=24.97
        y ~ x4 + x1 + x2
        
               Df Sum of Sq    RSS    AIC
        <none>              47.973 24.974
        + x3    1   0.10909 47.864 26.944
        
        Call:
        lm(formula = y ~ x4 + x1 + x2, data = df)
        
        Coefficients:
        (Intercept)           x4           x1           x2  
            71.6483      -0.2365       1.4519       0.4161
        ```
        
        - 변수제거 없는 회귀분석부터 시작하며 AIC(값이 작을수록 좋음)가 감소 기준으로 종속변수가 가장 큰 영향을 주는 독립변수부터 포함시키며 더 이상 추가될 독립변수가 없을 때 중단하고 분석을 시작

---

## 3) 응용회귀분석

- 과대적합과 과소적합
    - 과대적합(overfitting)
        - 모델이 훈련 데이터에는 굉장히 적합하지만 일반성이 떨어지는 경우
        - 보통 다양성(분산)을 집중 및 존중해서 일반화시키지 못한 경우
    - 과소적합(underfitting)
        - 과대적합의 반대로 모델이 너무 단순해서 데이터를 설명하기 힘든 경우
        - 보통 편향에 집중하여 일반화의 오류를 범하고 있는 경우
    - 규제
        - 어떤 한도를 정하고 그 한도를 넘지 못하도록 제한하는 것
        - 규제의 대상 : 일반적인 선형 회귀에서는 기울기(가중치), 다항회귀에서는 특정 차수 등
        
- 정규화 선형회귀
    - 정규화(regularization)
        - 좋은 모델이란 훈련 데이터를 잘 설명하고, test data에 대한 예측 성능이 우수한 모델
        - 과적합 모델을 방지하기 위해 회귀계수 $\beta$에 규제를 통해 모델의 변화를 주는 것이 정규화의 기본 개념
        
    - 정규화항을 붙이는 회귀분석
        - 정규화항을 통해 모델이 미치는 차원의 수를 감소시킴
        - 대표적인 종류
            
            
            | 릿지(능형)회귀 | 라쏘회귀 | 엘라스틱넷 |
            | --- | --- | --- |
            | 일반적으로 영향을 거의 미치지 않는 특성에 대하여 0에 가까운 가중치를 주게 됨 | 덜 중요한 가중치를 완전히 제거, 가중치가 0이 됨 | 릿지와 라쏘를 혼합 |
            | 변수 선택 불가능 | 변수 선택 가능 | 변수 선택 가능 |
            | 변수 간 상관관계가 높은 상황에서 좋은 예측성능 | 변수 간 상관관계가 높은 상황에서 릿지에 비해 상대적 예측 성능이 떨어짐 | 상관관계 큰 변수를 동시에 선택/배제하는 특성 |
            | 정규화항=⁍ | 정규화항=⁍ | 정규화항=⁍ |
        - 람다 값의 크기 증감이 미치는 영향 확인
            
            ```r
            install.packages('ridge')
            library(ridge)
            data('longley')
            names(longley)[1] = 'y'
            mod = linearRidge(y~.-1, data=longley, lambda = 'automatic')
            options(scipen=999)
            summary(mod)
            install.packages('genridge')
            library(genridge)
            lambda=c(0,0.005,0.01,0.02,0.04,0.08)
            r = ridge(y~.,longley,lambda=lambda)
            traceplot(r)
            ```
            
            ![Untitled](3%202%202%20%E1%84%80%E1%85%B5%E1%84%8E%E1%85%A9%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2086caee82180445e5b95b81ec3ce59d14/Untitled%203.png)
            
            - 람다값이 클수록 베타값들은 0에 가까워짐, 즉 규제가 많아짐, 적은 변수로 해석이 쉬워지지만 underfitting됨
            - 람다값이 작아질 수록 규제가 적음, 많은 변수로 해석이 어려워지고 결국 overfitting됨