# 3.3.2. 모형평가

## 1) 모형평가란?

- 구축된 모형이 임의의 모형보다 더 우수한 성과를 보이는지, 고려된 서로 다른 모형들 중 어느 것이 가장 우수한 예측 및 분류 성과를 보유하고 있는지 등을 비교 분석하는 과정

- 필요성
    - 분석 모형에는 다양한 알고리즘 및 방법론이 존재할 뿐만 아니라 하나의 방법론에도 다른 분류 결과를 초래하는 선택사항이 존재
    - 따라서 다양한 분석 모형 중에서 데이터 마이닝의 목적 및 데이터의 특성에 따라 가장 적합한 모형을 선택하기 위해서는 성과평가 기준이 필요
    
- 기준
    - 일반화의 가능성
        - 같은 모집단 내의 다른 데이터에 적용할 경우에도 안정적인 결과를 제공하는 것을 의미
        - 데이터를 확장하여 적용할 수 있는지에 대한 평가 기준
    - 효율성
        - 모형이 얼마나 효과적으로 구축되었는지를 평가
        - 적은 입력변수를 필요로 할수록 효율성은 높음
    - 예측과 분류의 정확성
        - 안정적이고 효율적인 모형을 구축하였다 하더라도 실제 문제에 적용했을 때 정확하지 못한 결과만을 양산한다면 그 모형은 의미를 가질 수 없음

---

## 2) 교차검증(cross validation)

- 모델을 만드는 것은 가지고 있는 샘플 데이터를 이용하여 충분한 정확도로 일반화시켜야 함
- 이를 위해 샘플 데이터를 training/validation/test dataset으로 나누어 모형의 성과를 검증
    - 학습데이터 : 훈련용 데이터, 분류기를 만들 때 사용하는 데이터이며 모델 학습에 사용
    - 검증데이터 : 구축된 모형의 과대추정 또는 과소추정을 미세조정하는데 활용하며, 분류기의 parameter 값을 최적화하기 위해 사용하는 데이터
    - 평가데이터 : 모형의 구축과는 상관없는 외부데이터이며, 모델의 성능을 검증하기 위한 데이터
- 주어진 데이터에서만 높은 성과를 보이는 모형의 과적합 문제를 해결하기 위한 단계로 잘못된 가설을 가정하게 되는 2종 오류의 발생을 방지
- 대표적인 추출방법
    - 홀드아웃(hold-out)
        - 주어진 원천 데이터를 랜덤하게 두 분류로 분리하여 교차검정을 실시하는 방법
        - 하나는 학습데이터로 하나는 테스트데이터로 사용
        - 일반적으로 학습용:테스트용=7:3, 8:2 등으로 나눔
        - 테스트 데이터의 결과는 모형에 영향을 미치지 않고 성과 측정만을 위하여 사용함
    - k-fold 교차검증
        - 전체 데이터를 사이즈가 동일한 k개의 하부집합으로 나누고 k번째의 하부집합을 테스트 데이터로, 나머지 k-1개의 하부집합을 학습 데이터로 사용
        - 이를 k번 반복하여 측정하고 각각의 반복측정 결과의 평균값을 최종 평가로 사용
        - 일반적으로 10-fold 교차검증이 사용됨
    - 붓스트램(bootstrap)
        - 주어진 자료에서 단순 랜덤 복원추출 방법을 활용하여 동일한 크기의 표본을 여러개 생성하는 복원 추출법
        - 평가를 반복한다는 측면에서 k-fold 교차검증과 유사하나 학습 데이터를 반복 재산정한다는 점에서 차이점이 존재
        - 전체 데이터의 양이 크지 않은 경우 모형 평가에 가장 적합
        - 학습데이터용 샘플을 뽑을 때, 복원추출 후 포함되지 않은 표본은 테스트 데이터로 사용함

---

## 3) 분류모형 평가 지표

- 오분류표(혼동행렬, confused matrix)
    - 목표변수의 실제 범주와 모형에 의해 예측된 분류 범주 사이의 관계를 나타내는 교차표 형태로 정리한 행렬
        
        
        |  | 예측 positive | 예측 negative |
        | --- | --- | --- |
        | 실제 positive | TP(true positive) | FN(false negative) |
        | 실제 negative | FP(false positive) | TN(true negative) |
    - 오분류표를 활용한 평가지표
        
        
        | 평가지표 | 계산식 | 설명 |
        | --- | --- | --- |
        | 정확도(accuracy) | ⁍ | 실제 분류 범주를 정확하게 예측한 비율 |
        | 오분류율(error rate) | ⁍ | 실제 분류 범주를 잘못 분류한 비율 |
        | 민감도(sensitivity, =recall(재현율)) | ⁍ | - 실제 positive 중 positive 예측
        - TP rate라고도 함
        - 범주의 불균형 문제에 사용 |
        | 특이도(specificity) | ⁍ | - 실제 negative 중 negative 예측
        - 범주의 불균형 문제에 사용 |
        | FP rate | ⁍ | - 실제 negative 중에서 positive 예측
        - 1-특이도 |
        | 정밀도(precision) | ⁍ | - 예측 positive 중에서 positive 실제 |
        | F1 | ⁍ | - 정밀도와 민감도를 하나로 합한 지표
        - 0~1 사이의 범위
        - 정밀도와 민감도 모두 클때 큰 값 |
        | ⁍ | ⁍ | - 재현율(민감도)과 정밀도를 평균낼 때 가중치 사용 |
        | 카파 통계량 | ⁍ | - 두 관찰자가 특정한 범주 값에 대한 일치도를 측정
        - 0~1의 값을 가지며 1에 가까울수록 모델의 예측값과 실제값이 정확히 일치 |
        - 범주의 불균형
            - 구분할 각 분류에 해당하는 데이터의 비율이 5:5가 아닐 경우, 학습데이터 내 비율이 높은 분류 쪽으로 결과를 내놓는 모델이 될 가능성이 존재
            - 이런 현상을 클래스(범주) 불균형이라고 함
            - 이를 해결하는 방법
                - 관찰 데이터가 적은 쪽의 데이터에 더 큰 가중치를 줌
                - 데이터가 많은 쪽에 under sampling, 데이터가 적은 쪽에 over sampling
                
- ROC 그래프(receiver operating characteristic graph)
    - 두 분류분석 모형의 결과를 시각화할 수 있다는 점에서 유용한 도구
    - x축에는 FP ratio(1-특이도), y축에는 민감도(sensitivity=recall)를 나타냄
    - 각각 FPR(거짓긍정룔, x축), TPR(참긍정률, y축)이라고도 부름
    - 모형의 성과를 평가하는 기준은 ROC 그래프의 밑부분 면적인 AUC(area under curve)이며, AUC가 클수록 좋은 모형으로 평가(0.5~1의 범위를 가짐)
    - 가장 이상적으로 완벽한 모형의 경우, x축은 0, y축은 1의 값을 보이며 AUC(area under curve)가 1로 도출됨
    - infert data를 신경망 모형과 의사결정 모형을 통해 모형 평가해보기
    
    ```r
    set.seed(1234)
    infert <- infert[sample(nrow(infert)),] # random suffle 
    infert <- infert[,c("age","parity","induced","spontaneous","case")]
    trainData <- infert[1:(nrow(infert)*0.7),]
    testData <- infert[((nrow(infert)*0.7)+1):nrow(infert),]
    # neural network
    
    library(neuralnet)
    net.infert <- neuralnet(case~age+parity+induced+spontaneous, data=trainData,hidden=3,
                            err.fct="ce",linear.output = FALSE,likelihood=TRUE)
    n_test <- subset(testData,select=-case)
    nn_pred<-compute(net.infert, n_test)
    testData$net_pred <- nn_pred$net.result
    head(testData)
    
    # Decision Tree
    install.packages('C50')
    library(C50)
    trainData$case <- factor(trainData$case)
    dt.infert <- C5.0(case~age+parity+induced+spontaneous, data=trainData)
    testData$dt_pred <-predict(dt.infert, testData, type="prob")[,2]
    head(testData)
    
    # ROC curve packages
    install.packages("Epi")
    library(Epi)
    
    # Neural Network ROC
    neural_ROC <- ROC(form=case~net_pred, data=testData, plot="ROC")
    
    # Decision Tree ROC
    dtree_ROC <- ROC(form=case~dt_pred, data=testData, plot="ROC")
    ```
    
    ![Untitled](3%203%202%20%E1%84%86%E1%85%A9%E1%84%92%E1%85%A7%E1%86%BC%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%A1%20acdc615d3e454463a47eb9be62ab6cd2/Untitled.png)
    
    ![Untitled](3%203%202%20%E1%84%86%E1%85%A9%E1%84%92%E1%85%A7%E1%86%BC%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%A1%20acdc615d3e454463a47eb9be62ab6cd2/Untitled%201.png)
    
- 이익도표
    - 이익(gain)은 목표 범주에 속하는 개체들이 각 등급에 얼마나 분포하고 있는지를 나타내는 값으로, 해당 등급에 따라 계산된 이익값을 누적으로 연결한 도표가 이익도표임
    - 분류분석 모형을 사용하여 분류된 관측치가 각 등급별로 얼마나 포함되는지를 나타내는 도표
    - 통계량
        
        
        | 구분 | 설명 |
        | --- | --- |
        | %Captured Response | ⁍ |
        | %Response | ⁍ |
        | Baseline Lift | ⁍ |
        | Lift | ⁍ |
        
- 향상도 곡선(Lift curve)
    - 랜덤모델과 비교하여 해당 모델의 성과가 얼마나 향상되었는지를 각 등급별로 파악하는 그래프
    - 상위 등급에서 향상도가 매우 크고 하위 등급으로 갈수록 향상도가 감소하게 되며, 일반적으로 이러한 모형은 예측력이 적절함을 의미함
    - 등급에 관계없이 향상도가 차이가 없으면 모형의 예측력이 좋지 않음을 나타냄
    - prediction() 함수를 사용해 ROC 그래프 및 향상도 그래프의 자료를 준비하고 신경망 모형과 의사결정나무 모형의 ROC 그래프를 performance()함수를 사용해 비교해보기
        
        ```python
        install.packages("ROCR")
        library(ROCR)
        n_r <- prediction(testData$net_pred, testdata$case)
        d_r <- prediction(testData$dt_pred, testdata$case)
        n_p <- performance(n_r,'tpr','fpr')
        d_p <- performance(d_r,'tpr','fpr')
        plot(n_p,col='red')
        par(new=TRUE)
        plot(d_p,col='blue')
        abline(a=0,b=1)
        
        n_lift <- performance(n_r,"lift","rpp")
        plot(n_lift,col="red")
        abline(v=0.2)
        ```
        

![Untitled](3%203%202%20%E1%84%86%E1%85%A9%E1%84%92%E1%85%A7%E1%86%BC%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%A1%20acdc615d3e454463a47eb9be62ab6cd2/Untitled%202.png)

![Untitled](3%203%202%20%E1%84%86%E1%85%A9%E1%84%92%E1%85%A7%E1%86%BC%E1%84%91%E1%85%A7%E1%86%BC%E1%84%80%E1%85%A1%20acdc615d3e454463a47eb9be62ab6cd2/Untitled%203.png)