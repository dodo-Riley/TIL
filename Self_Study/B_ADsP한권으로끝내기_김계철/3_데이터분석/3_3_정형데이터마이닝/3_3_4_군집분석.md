# 3.3.4. 군집분석

## 1) 군집분석의 개요

- 집단 또는 범주에 대한 사전 정보가 없는 데이터의 경우, 주어진 관측값을 사용하여 전체를 몇 개의 유사한 집단으로 그룹화하여 각 집단의 성격을 파악하기 위한 기법
- 군집분석에 이용되는 다변량 자료는 별도의 반응변수가 요구되지 않으며, 오로지 개체들 간의 유사성에만 기초하여 군집을 형성함
- 군집분석은 이상값 탐지에도 사용되며, 심리학, 사회학, 경영학, 생물학 등 다양한 분야에 이용되고 있음

## 2) 군집분석의 종류

![Untitled](3%203%204%20%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203fd20f56102942d69213e83f27938491/Untitled.png)

![Untitled](3%203%204%20%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203fd20f56102942d69213e83f27938491/Untitled%201.png)

- 계층적 군집(hierarchical clustering)
    - 가장 유사한 개체를 묶어나가는 과정을 반복하여 원하는 개수의 군집을 형성하는 방법
    - 보통 계통도, 덴드로그램의 형태로 결과가 주어지며 각 개체는 하나의 군집에만 속하게 됨
        - 덴드로그램의 형태를 통해 군집들 간의 구조적 관계를 쉽게 살펴볼 수 있음
        - 항목 간의 거리, 군집 간의 거리를 알 수 있고, 군집 내의 항목 간 유사 정도를 파악함으로써 군집의 견고성을 해석할 수 있음
    - 작은 군집으로부터 출발하여 형성해 나가는 병합적(agglomerative) 방법과, 큰 군집으로부터 출발하여 분리해 나가는 분할적(divisive) 방법이 있음
        - 병합적 방법에서는 한 번 군집이 형성되면 군집에 속한 개체는 다른 군집으로 이동할 수 없음
    - 군집을 형성하는 데 매 단계에서 지역적 최적화를 수행해 나가는 방법을 사용하므로, 그 결과가 전역적 최적해라고 볼 수 없음
    - 계층적 군집의 거리 측정 방법
        
        ![출처 : [https://codedragon.tistory.com/9829](https://codedragon.tistory.com/9829)](3%203%204%20%E1%84%80%E1%85%AE%E1%86%AB%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203fd20f56102942d69213e83f27938491/Untitled%202.png)
        
        출처 : [https://codedragon.tistory.com/9829](https://codedragon.tistory.com/9829)
        
        - 단일연결법 : 한 군집의 점과 다른 군집의 점 사이의 가장 짧은 거리, 사슬모양으로 생길 수 있으며 고립된 군집을 찾는 데 중점을 둔 방법
        - 완전연결법 : 각 군집에서 하나씩 관측값을 뽑았을 때 나타날 수 있는 거리의 최대값을 측정, 군집들의 내부응집성에 중점을 둔 방법
        - 평균연결법 : 모든 항목에 대한 거리 평균을 구하면서 군집화를 하기 때문에 계산량이 불필요하게 많아질 수 있음
        - 중심연결법 : 두 군집이 결합할 때 새로운 군집의 평균은 가중 평균을 통해 구해짐
        - 와드연결법 : 군집 내의 오차제곱합에 기초하여 군집을 수행
    - 계층적 군집의 거리
        - 연속형 변수의 경우
            - 수학적 거리
                - 유클리드(Euclidean) 거리 : 두 점 간 차를 제곱하여 모두 더한 값의 양의 제곱근
                - 맨해튼(Manhattan) 거리 : 시가(city-block)이라고도 불리며, 두 점 간 차의 절대값을 합한 값
                - 민코프스키(Minkowski) 거리 : m차원 민코프스키 공간에서의 거리이며 m=1일때 맨해튼거리, m=2일 때 유클리디안 거리와 같음, $d(i,j)=[\sum_{f=1}^p (x_{if}-x_{jf})^m]^{1/m}$
            - 통계적 거리
                - 표준화(standardized) 거리 : 변수의 측정단위를 표준화한 거리
                - 마할라노비스(Mahalanobis) 거리 : 변수의 표준화와 함게 변수 간의 상관성을 동시에 고려한 통계적 거리
        - 명목형 자료의 경우
            - 단순 일치 계수(simple matching coefficient) : 전체 속성 중에서 일치하는 속성의 비율
            - 자카드 계수(Jaccard coefficient) : 두 집합 사이의 유사도를 측정하는 방법으로 전체 합집합에서 교집합의 비율, 0~1의 값을 가짐
        - 순서형 자료의 경우
            - 순위상관계수(rank correlation coefficient) : 스피어만 상관계수를 이용해 거리를 측정
        - 기타
            - 캔버라 거리 : 가중치 있는 맨해튼 거리, 원점 주변에 흧어져 있는 데이터에 주로 사용
            - 체비셰프 거리 : 체스판 거리라고도 하며 두 점의 x좌표 파이와 y좌표 차이 중 큰 값을 찾는 거리
            - 코사인 유사도 : 두 벡터의 내적을 각 벡터의 크기로 나눈값
    - 계층적 군집분석의 장단점
        - 장점
            - 덴드로그램을 통해 군집화 결과를 표현하며, 설명 및 해석이 가능함
            - 군집의 수를 명시할 필요가 없음
        - 단점
            - 데이터 집합이 매우 클 경우 계산속도가 느림
            - 이상치 값에 민감
            - 한 번 군집이 형성되면 군집에 속한 개체는 다른 군집으로 이동할 수 없음
        
- 비계층적 군집
    - K-means clustering
        - 원하는 군집 수만큼 초기값을 지정하고, 각 개체를 가까운 초기값에 할당하여 군집을 형성한 뒤, 각 군집의 평균을 재계산하여 초기값을 갱신
        - 갱신된 값에 대해 위의 할당 과정을 반복하여 k개의 최종 군집을 형성하는 방법
        - k 평균 군집 과정
            - 초기 군집 중심으로 k개의 객체를 임의로 선택
            - 각 자료를 가장 가까운 군집 중심에 할당, 자료들의 군집의 중심점으로부터 오차제곱합이 최소가 되도록 각 자료를 할당
            - 각 군집내의 자료들의 평균을 계산하여 군집의 중심을 갱신
            - 군집 중심의 변화가 없을 때까지 반복
        - 장단점
            - 장점
                - 알고리즘이 단순하며, 바르게 수행되어 계층적 군집보다 많은 양의 자료를 처리
                - 거의 모든 형태의 데이터에 적용이 가능
                - 주어진 데이터의 내부 구조에 대한 사전적 정보 없이 의미 있는 자료로 분석이 가능
            - 단점
                - 잡음이나 이상치에 영향을 받기 쉬움
                - 사전에 군집의 수를 정해주어야 함
                - U 형태의 군집이 존재할 경우에는 성능이 떨어짐
            
- 군집분석의 타당성 지표
    - 군집분석은 지도학습이 아니기 때문에 일반적인 머신러닝 알고리즘처럼 정확도 등을 지표로 평가할 수 없음
    - 결과가 유용한지 따지는 군집타당성지표가 존재
    - 군집 타당성 지표는 군집간 거리, 군집의 지름, 군집의 분산 등을 이용
        - Dunn index
            - 군집 간 거리의 최소값을 분자로, 군집 내 요소 간 거리의 최대값을 분모로 하는 지표
            - 군집 간 거리는 멀수록, 군집 내 분산은 작을수록 좋은 군집화 결과이며 이 경우 Dunn undex는 커지게 됨
        - 실수엣 계수(Silhouette coefficient)
            - 각 데이터 포인트와 주위 데이터 포인트들과의 거리 계산을 통해 구한 값
            - 구하는 과정
                - 한 군집에서 데이터 포인트를 선택
                - 해당 군집 내에서 다른 데이터 포인트와의 거리 평균을 구함
                - 주변 군집 간 포인트와의 거리 평균을 구함
                - (여러 거리 평균 중 가장 큰 군집 간 거리의 평균-군집 내 거리 평균)의 값을 가장 큰 군집 간 거리의 평균으로 나눈 값이 실루엣 계수
            - 실루엣 계수의 평균값이 1에 가까울수록 군집화가 잘 되었다고 생각할 수 있음
            - 일반적으로 실루엣 계수가 0.5보다 크면 결과가 타당한 것으로 판단
            - 장단점
                - 장점
                    - 클러스터링 알고리즘에 영향을 받지 않음
                    - 적절한 클러스터 개수를 정하거나 더 나은 기법을 선택하는 기준으로 삼을 수 있음
                    - 결과값을 시각화 할 수 있음
                - 단점
                    - 데이터 양이 많아질수록 수행시간이 오래 걸림

## 3) 혼합분포 군집

- 혼합분포 군집의 개요
    - 모형기반의 군집방법
    - 데이터가 k개의 모수적 모형(정규분포 또는 다변량 정규분포를 가정)의 가중합으로 표현되는 모집단 모형으로 나왔다는 가정하에서 모수와 함께 가중치를 자료로부터 추정하는 방법을 사용
    - k개는 각 모형의 군집을 의미, 각 데이터는 추정된 k개의 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집의 분류가 이루어짐
    
- EM 알고리즘
    - 초기에 k 평균 군집처럼 랜덤하게 초기화
    - 각 관측치는 각각의 군집에 속할 확률이 계산(E-step)
    - 각 자료가 어느 집단에 속하는지에 대한 정보를 가지는 변수를 잠재변수(latent variable)이라고 함
    - 계산된 확률을 이용해서 최대우도추정으로 모수를 다시 추정하고, 이를 반복하면서 모수의 값이 변화가 없을때까지 반복(M-step)
        - 최대 우도 추정(maximum likelihood estimation) : 어떤 확률변수에서 수집한 값들을 토대로 그 확률변수의 모수를 구하는 방법으로, 어떤 모수가 주어졌을 때 원하는 값들이 나올 가능도를 최대로 만드는 모수를 선택하는 방법
        - 로그 가능도(log likelihood) : 최대우도에 로그함수를 취해서 단순화시킨 값, 계산의 편이성을 위해서 사용
        
- 혼합분포군집의 장단점
    - 장점
        - k평균 군집의 절차와 유사하지만, 확률분포를 도입하여 군집을 수행
        - 군집을 몇개의 모수로 표현할 수 있으며, 서로다른 크기나 모양의 군집을 찾을 수 있음
        - 이상치 탐지 기법으로 활용 가능
    - 단점
        - 데이터가 커지면 수렴에 시간이 걸릴 수 있음
        - 군집의 크기가 너무 작으면 추정의 정도가 떨어지거나 어려울 수 있음
        - 이상치 자료에 민감하므로 사전에 조치 필요

## 4) SOM(self organizing maps)

- SOM 개념
    - SOM, 또는 SOFM은 인공신경망의 한 종류로서 기본 개념은 1980년대 핀란드 교수인 Teuvo Kohonen이 제안한 Kohonen Network에 근간을 두고 있음
    - 차원축소와 군집화를 동시에 수행하는 기법인 자기조직화지도는 입력 벡터를 훈련 집합에서 매치되도록 가중치를 조정하는 인공 신경세포 격자에 기초한 자율학습의 한 방법
    
- SOM 구조
    - 입력층 : 입력 벡터를 입력받는 층
    - 경쟁층 : 입력 벡터의 특성에 따라 입력 벡터가 한 점으로 클러스터링되는 층
    - 가중치 : 인공신경망에서 가중치는 각 입력값에 대한 중요도 값을 의미
    - 노드 : 경쟁층에서 입력 벡터들이 서로의 유사성에 의해 모이는 하나의 영역
    
- SOM process
    - SOM 맵의 노드에 대한 연결 강도로 초기화
    - 입력 벡터와 경쟁층 노드 간의 유클리드 거리를 계산하여 입력 벡터와 가장 짧은 노드를 선택
    - 선택된 노드와 이웃 노드의 가중치를 수정
    - 이를 반복하며 가중치는 입력 패턴과 가장 유사한 경쟁층 뉴련이 승자가 되며, 승자 독식 구조로 경쟁층에는 승자 노드만이 나타남
    
- SOM 장단점
    - 장점
        - 구조상 수행이 상당히 빠름
        - 여러 단계의 피드백이 아닌 단 하나의 feedforward flow를 사용
        - 실시간 학습 처리를 할 수 있는 모델
    - 단점
        - 수치형 데이터 변수에서만 사용 가능
        - 범주형 자료는 더미변수로 변환하여 사용해야함

## 5) 밀도기반군집(density based clustering)

- 밀도기반 군집 개요
    - 주변 밀도가 높은 각 데이터가 서로 가까이 위치하면 동일한 군집으로 묶이게 되는 방식을 사용
    - 거리기반 군집방법들을 일반적으로 구형의 군집만을 찾지만, 밀도기반군집은 임의의 형태의 군집을 찾을 수 있음
    
- DBSCAN 알고리즘
    - 2개의 파라미터가 필요
        - eps(거리)와 minPts(조밀영역을 구성하는 데 필요한 개체의 최소수)
    - 프로세스
        - eps와 minPts 설정
        - 노이즈를 군집에서 제외
        - eps 반경 안에 있는 코어점들을 서로 연결
        - 연결된 코어점들을 하나의 군집으로 형성
        - 경계점은 관련된 코어점을 포함하는 군집 중 하나에 할당
    - 장단점
        - 장점
            - 군집의 수를 미리 정할 필요가 없음
            - 임의의 형태를 가지는 군집을 찾을 수 있음
            - 노이즈 자료에 대한 정보를 제공하여 이상치에 민감하지 않음
            - 2개의 파라미터만 요구되며 데이터베이스 값들의 순서에는 민감하지 않음
        - 단점
            - 경계점은 두 군집 모두에 속할 가능성이 존재