# 3.3.3. 분류분석

- 반응변수가 범주형인 경우의 예측모형은 새로운 자료에 대한 분류가 주목적이며, 연속형인 경우에는 그 값을 예측하는 것이 주목적이라고 할 수 있음

## 1) 로지스틱 회귀모델(logistic regression)

- 로지스틱 회귀분석 개념
    - 회귀분석은 기본적으로 종속변수와 독립변수가 모두 등간척도 이상으로 측정된 경우에 적용되는 통계기법
        - 만약 독립변수가 명목이나 서열척도로 측정된 경우, 독립변수를 더미변수로 전환하여 회귀분석을 적용
        - 종속변수가 질적인 척도, 즉 명목척도로 측정된 경우에는 회귀분석의 적용이 어려워짐
    - 명목척도로 측정된 종속변수를 독립변수들을 이용하여 예측하고자 하는 경우, 로지스틱회귀분석과 판별분석(discriminant analysis)이 사용될 수 있음
        - 이 두 모형의 기본 원리는 관찰값이 어떤 집단에 속하는지를 직접 예측하지 않고, 독립변수들로 구성된 식을 이용하여 종속변수값를 예측한 뒤 이 값을 토대로 관찰값이 어느 집단에 속하는지를 예측하게 됨
        - 두 모형의 차이점은 판별분석은 독립변수들의 정규성과 공분산이 동일해야한다는 가정이 필요하지만, 로지스틱 회귀분석은 이러한 가정을 요구하지 않음
        - 로지스틱 회귀분석은 각 관찰치가 특정 집단에 속할 확률을 토대로 그 관찰자가 어느 집단에 속하는지 예측하는 반면, 판별분석에서는 판별점수를 이용하여 관찰치가 어느 집단에 속하는지 예측함
        
- 로지스틱 회귀모형
    - 반응변수가 범주형인 경우에 적용되는 회귀분석 모형
    - 종속변수와 독립변수 간의 관계식을 이용하여 두 집단을 분류하는데 사용
    - 선형회귀모형과의 차이점
        - 이항 데이터에 적용하였을 때, 종속변수의 결과가 0~1의 범위로 제한
        - 종속변수가 이항이기 때문에 조건부 확률의 분포가 정규분포 대신 이항 분포를 따름
    - 오즈(odds)와 로짓변환
        - 오즈
            - 확률 p가 주어졌을 때, 사건이 발생할 확률이 사건이 발생하지 않을 확률의 몇 배 인지에 대한 개념
            - 오즈=사건이 발생할 확률/사건이 발생하지 않을 확률
            - 오즈는 음이 아닌 실수값으로, 성공이 일어날 가능성이 높은 경우에는 1.0보다 큰 값을, 반대는 1.0보다 작은 값은 가지게 됨
        - 로짓변환( 참고 : [https://nittaku.tistory.com/478](https://nittaku.tistory.com/478))
            - 로지스틱 회귀는 반응변수가 범주형이므로 선형회귀 방식으로 fitting하기 어려움
            - 따라서 fitting하기 위해 사용하는 것이 오즈에 로그를 취하는 로짓변환이라고하며 로짓은 $ln(p/(1-p))$임
            - 선형회귀는 $Y=\beta_0+\beta_1X$에서 좌변과 우변이 모두 음의 무한대에서 양의 무한대까지의 값을 가질 수 있지만, 로지스틱 회귀에서는 좌변은 0,1 같은 범주값이고 우변은 무한대의 값을 가질 수 있음
            - 다시 말해, 로지스틱회귀에서 선형회귀와 같은 방법을 그대로 적용한다면, 좌변과 우변의 값이 맞지 않음
            - 따라서 이 식을 같게하기 위해 p를 예측하는 것이 아니라 오즈를 예측하도록 함
            - p는 0~1사이의 값을 가지므로 오즈의 범위가 0을 초과해 양의 무한대까지 값을 가질 수 있으며, 다시 여기에 로짓변환을 하면 음의 무한대에서 양의 무한대 값을 가질 수 있게됨
            - 결국, 독립변수가 1개인경우, 로지스틱 회귀모형은 $Y=ln(p/(1-p))=\beta_0+\beta_1X$로 표현됨
            - 표현된 식을 풀어 쓰면 $E(Y) = p ={{e^{\beta_0+\beta_1X}}\over{1+e^{\beta_0+\beta_1X}}}$ 이 되고 이를 그래프로 그리면 아래와 같음
                
                ![Untitled](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled.png)
                
            - 이 곡선을 시그모이드 함수하고 하며, 단순 로지스틱인 경우 로지스틱 회귀계수 $\beta_1$이 0보다 크면 S자모양을 그리고 0보다 작으면 역 S자 모양을 그림
            - 로지스틱 분포의 누적함수는 곧 성공의 확률로 추정할 수 있으며, 이는 로지스틱 회귀가 분류 목적으로 사용될 경우 기준값보다 크면 Y=1인 집단으로, 작으면 Y=0인 집단으로 분류하게 됨
    - 로지스틱 회귀계수의 이해
        - $p/(1-p)=exp(\beta_0) \times exp(\beta_1X)$
        - X를 1단위 증가시키게 되면 오즈의 예측값은 $exp(\beta_1)$만큼 곱해지게 됨
        - 따라서 로지스틱 회귀분석에서의 회귀계수는 해당 변수가 1 증가함에 따른 오즈의 변화량을 의미
        
- 선형회귀분석 vs 로지스틱 회귀분석
    
    
    |  | 일반선형 회귀분석 | 로지스틱 회귀분석 |
    | --- | --- | --- |
    | 종속변수 | 연속형 변수 | 이산형 변수 |
    | 모형 탐색 방법 | 최소자승법 | 최대우도법, 가중최소자승법 |
    | 모형 검정 | F-test, t-test | 카이제곱 test |
    - 예제 : 단순선형회귀식 추정
        
        ```r
        > x = c(27.2,27.7,28.3,28.4,29.9) # 온도
        > male = c(2,17,26,19,27) # 수컷 수
        > female = c(25,7,4,8,1) # 암컷 수
        > total = male+female
        > pmale = male/total
        > z = lm(pmale~x) # 회귀분석(종속변수~독립변수)
        > summary(z)
        
        Call:
        lm(formula = pmale ~ x)
        
        Residuals:
               1        2        3        4        5 
        -0.29528  0.20532  0.20325  0.01356 -0.12685 
        
        Coefficients:
                    Estimate Std. Error t value Pr(>|t|)
        (Intercept)  -6.9020     3.4737  -1.987    0.141
        x             0.2673     0.1227   2.179    0.117
        
        Residual standard error: 0.2496 on 3 degrees of freedom
        Multiple R-squared:  0.6128,	Adjusted R-squared:  0.4838 
        F-statistic: 4.748 on 1 and 3 DF,  p-value: 0.1175
        
        > p = coefficients(z)[1]+coefficients(z)[2]*x
        > p
        [1] 0.3693497 0.5030147 0.6634127 0.6901457 1.091140
        ```
        
        - 추정 회귀식은 수컷비율 = -6.9020+0.2673*온도
        - 온도에 따른 수컷 비율 예측(확률) 결과값이 1보다 큰 값이 존재
        
    - 예제 : 로짓변환된 값을 종속변수로 단순 선형 회귀식 추정
        
        ```r
        > logit = log(pmale/(1-pmale))
        > z1 = lm(logit~x)
        > summary(z1)
        
        Call:
        lm(formula = logit ~ x)
        
        Residuals:
              1       2       3       4       5 
        -1.3837  1.1107  0.9930 -0.1976 -0.5224 
        
        Coefficients:
                    Estimate Std. Error t value Pr(>|t|)  
        (Intercept) -51.1122    16.9415  -3.017   0.0569 .
        x             1.8371     0.5983   3.070   0.0545 .
        ---
        Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
        
        Residual standard error: 1.217 on 3 degrees of freedom
        Multiple R-squared:  0.7586,	Adjusted R-squared:  0.6781 
        F-statistic: 9.428 on 1 and 3 DF,  p-value: 0.05454
        
        > logit2 = coefficients(z1)[1]+coefficients(z1)[2]*x
        > rmalehat = exp(logit2)/(1+exp(logit2))
        > rmalehat
        [1] 0.2419512 0.4443709 0.7065822 0.7431787 0.9785063
        ```
        
        - 로짓변환하여 온도별 예측 확률값은 0~1 사이의 값을 가짐
    
    - 예제 : 최대우도추정법을 사용해 회귀방정식 추정
        
        ```r
        > logit = glm(pmale~x,family="binomial", weight=total)
        > summary(logit)
        
        Call:
        glm(formula = pmale ~ x, family = "binomial", weights = total)
        
        Deviance Residuals: 
             1       2       3       4       5  
        -2.224   2.248   1.239  -1.382  -1.191  
        
        Coefficients:
                    Estimate Std. Error z value Pr(>|z|)    
        (Intercept) -61.3183    12.0224  -5.100 3.39e-07 ***
        x             2.2110     0.4309   5.132 2.87e-07 ***
        ---
        Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
        
        (Dispersion parameter for binomial family taken to be 1)
        
            Null deviance: 64.429  on 4  degrees of freedom
        Residual deviance: 14.863  on 3  degrees of freedom
        AIC: 33.542
        
        Number of Fisher Scoring iterations: 5
        
        > exp(-61.3183)*exp(2.2111*27)
        [1] 0.198176
        > exp(-61.3183)*exp(2.2111*28)
        [1] 1.808504
        ```
        
        - 추정회귀식은 수컷비율=-61.318+2.211*온도
        - 경계값은 수컷비율이 0이되는 온도인 27.3도
        - 회귀계수를 해석하면,  28도에서 오즈 예측값은 27도에서의 예측값보다 exp(2.211)=9.125배
        
    - 예제 : iris data를 활용한 로지스틱회귀분석
        
        ```r
        > colnames(iris) = tolower(colnames(iris))
        > a = subset(iris,species=='setosa'|species=='versicolor')
        > a$species = factor(a$species)
        > b = glm(species~sepal.length,data=a,family=binomial)
        > summary(b)
        
        Call:
        glm(formula = species ~ sepal.length, family = binomial, data = a)
        
        Deviance Residuals: 
             Min        1Q    Median        3Q       Max  
        -2.05501  -0.47395  -0.02829   0.39788   2.32915  
        
        Coefficients:
                     Estimate Std. Error z value Pr(>|z|)    
        (Intercept)   -27.831      5.434  -5.122 3.02e-07 ***
        sepal.length    5.140      1.007   5.107 3.28e-07 ***
        ---
        Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
        
        (Dispersion parameter for binomial family taken to be 1)
        
            Null deviance: 138.629  on 99  degrees of freedom
        Residual deviance:  64.211  on 98  degrees of freedom
        AIC: 68.211
        
        Number of Fisher Scoring iterations: 6
        
        > coef(b)
         (Intercept) sepal.length 
          -27.831451     5.140336 
        > cdplot(species~sepal.length,data=a)
        ```
        
        ![Untitled](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%201.png)
        
        - sepal.length p-value 유의수준보다 낮아 매우 유의한 변수
        - 로지스틱 회귀계수 값은 exp(5.140336)의 값이므로 약 170배
        - cdplot()은 연속형 변수의 변화에 따른 범주형 변수의 조건부 분포를 보여줌, sepla.length가 커짐에 따라 versicolor의 확률이 증가함을 볼 수 있음
        
    - 예제 : 이항변수 vs를 반응변수로, mpg와 am을 예측변수로 하는 로지스틱 회귀모형 추정
        
        ```r
        > glm.vs = glm(vs~mpg+am,data=mtcars, family="binomial")
        > summary(glm.vs)
        
        Call:
        glm(formula = vs ~ mpg + am, family = "binomial", data = mtcars)
        
        Deviance Residuals: 
             Min        1Q    Median        3Q       Max  
        -2.05888  -0.44544  -0.08765   0.33335   1.68405  
        
        Coefficients:
                    Estimate Std. Error z value Pr(>|z|)   
        (Intercept) -12.7051     4.6252  -2.747  0.00602 **
        mpg           0.6809     0.2524   2.698  0.00697 **
        am           -3.0073     1.5995  -1.880  0.06009 . 
        ---
        Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
        
        (Dispersion parameter for binomial family taken to be 1)
        
            Null deviance: 43.860  on 31  degrees of freedom
        Residual deviance: 20.646  on 29  degrees of freedom
        AIC: 26.646
        
        Number of Fisher Scoring iterations: 6
        ```
        
        - 다중로지스틱에서 추정된 회귀계수 해석은 아래와 같음
            - am이 주어질 때 mpg값이 한 단위 증가할 때 vs=1 오즈가 exp(0.6809)=1.98배 증가
            - mpg가 주어질 대 오즈에 대한 am의 효과는 exp(-3.0073)=0.05배
            

## 2) 인공 신경망(Artificial Neural Network) 모형

- 인공 신경망 모형의 개념
    
    ![뉴런의 구조, x는 입력벡터의 값이며 w는 가중치, b는 편향, f는 활성함수, net값은 $\sum w_ix_i +b$   (출처 : [https://untitledtblog.tistory.com/141](https://untitledtblog.tistory.com/141))](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%202.png)
    
    뉴런의 구조, x는 입력벡터의 값이며 w는 가중치, b는 편향, f는 활성함수, net값은 $\sum w_ix_i +b$   (출처 : [https://untitledtblog.tistory.com/141](https://untitledtblog.tistory.com/141))
    
    - 동물의 뇌신경계를 모방하여 분류 또는 예측을 위해 만들어진 머신러닝 알고리즘
    - 입력(input)은 시냅스에 해당하며 개별 신호의 강도에 따라 가중(weight)되며, 활성함수(activation function)는 인공 신경망의 출력(output)을 계산함
    - 많은 데이터에 대해 학습을 거쳐, 원하는 결과가 나오도록(오차가 최소화되도록) 가중치가 조정됨
    - 용어 설명
        - 임계치(threshold) : 어떠한 값이 활성화되기 위한 최소값
        - 가중치(weight) : 퍼셉트론의 학습 목표는 학습 벡터를 두 부류로 선형 분류하기 위한 선형 경계를 찾는 것임. 가중치는 이러한 선형 경계의 방향성 또는 형태를 나타내는 값
        - 편향(bias) : 하나의 뉴런으로 입력된 모든 값을 다 더한 다음에(가중합) 이 값에 더해주는 상수. 이 값은 하나의 뉴런에서 활성화 함수를 거쳐 최종적으로 출력되는 값을 조절하는 역할을 함
        - net값 : 입력값과 가중치의 곱을 모두 합한 값으로써, 기하하적으로 해석하면 선형 경계의 방정식과 같음
        - 활성함수(activation function) : 뉴런에서 계산된 net값이 임계치보다 크면 1을 출력하고 작으면 0을 출력하는 함수. 이 정의는 단층 퍼셉트론에서만 유효하며, 다층 퍼셉트론에서는 다른 형태의 활성함수를 이용함
        - 뉴런(neuron) : 인공 신경망을 구성하는 가장 작은 요소

- 인공 신경망의 종류
    - 단층 퍼셉트론
        
        ![출처 : [https://blog.naver.com/PostView.naver?blogId=jevida&logNo=221849155477&redirect=Dlog&widgetTypeCall=true&directAccess=false](https://blog.naver.com/PostView.naver?blogId=jevida&logNo=221849155477&redirect=Dlog&widgetTypeCall=true&directAccess=false)](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%203.png)
        
        출처 : [https://blog.naver.com/PostView.naver?blogId=jevida&logNo=221849155477&redirect=Dlog&widgetTypeCall=true&directAccess=false](https://blog.naver.com/PostView.naver?blogId=jevida&logNo=221849155477&redirect=Dlog&widgetTypeCall=true&directAccess=false)
        
        - 입력층과 출력층으로만 구성
        - 입력층은 학습 벡터 도는 입력 벡터가 입력되는 계층으로서, 입력된 데이터는 출력층 뉴런으로 전달되어 활성함수에 따라 값이 출력
        - 학습 알고리즘
            
            ![출처 : [https://mblogthumb-phinf.pstatic.net/MjAxNzA2MTlfMTA3/MDAxNDk3ODQzMjEwMTE4.jXgcNwRTVRbJEDO6JsHOxtftq_h9VxtCjeuCvDd9nCEg.-cCAbXvpIdBqfpS4DkUmpm339EeH3GO3FgnHr95ytiwg.PNG.samsjang/캡처.PNG?type=w2](https://mblogthumb-phinf.pstatic.net/MjAxNzA2MTlfMTA3/MDAxNDk3ODQzMjEwMTE4.jXgcNwRTVRbJEDO6JsHOxtftq_h9VxtCjeuCvDd9nCEg.-cCAbXvpIdBqfpS4DkUmpm339EeH3GO3FgnHr95ytiwg.PNG.samsjang/%EC%BA%A1%EC%B2%98.PNG?type=w2)](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%204.png)
            
            출처 : [https://mblogthumb-phinf.pstatic.net/MjAxNzA2MTlfMTA3/MDAxNDk3ODQzMjEwMTE4.jXgcNwRTVRbJEDO6JsHOxtftq_h9VxtCjeuCvDd9nCEg.-cCAbXvpIdBqfpS4DkUmpm339EeH3GO3FgnHr95ytiwg.PNG.samsjang/캡처.PNG?type=w2](https://mblogthumb-phinf.pstatic.net/MjAxNzA2MTlfMTA3/MDAxNDk3ODQzMjEwMTE4.jXgcNwRTVRbJEDO6JsHOxtftq_h9VxtCjeuCvDd9nCEg.-cCAbXvpIdBqfpS4DkUmpm339EeH3GO3FgnHr95ytiwg.PNG.samsjang/%EC%BA%A1%EC%B2%98.PNG?type=w2)
            
            1. 가중치와 바이어스 가중치를 -0.5~0.5 사이의 임의값으로 초기화
            2. 하나의 학습 벡터에 대한 출력층 뉴런의 net값을 계산
            3. 활성함수를 통해 계산된 net값으로부터 뉴런의 실제 출력값을 계산
            4. 뉴런의 실제값과 예측값의 차이가 발생하면 리턴하여 가중치를 업데이트
            5. 뉴런의 실제값과 예측값이 차이가 없을 때까지 반복
        - 단층 퍼셉트론의 XOR 문제
            
            ![출처 : [http://wiki.hash.kr/index.php/퍼셉트론](http://wiki.hash.kr/index.php/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0)](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%205.png)
            
            출처 : [http://wiki.hash.kr/index.php/퍼셉트론](http://wiki.hash.kr/index.php/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0)
            
            - and와 or 게이트는 직선을 그어 결과값을 구별할 수 있으나, xor의 경우 불가능
            - 1969년 Marvin Minsky, Seymour Papert 두 사람이 ‘Perceptron’을 통해 퍼셉트론 모델은 선형분리 기능 밖에 없다는 사실을 입증
            - 이를 해결한 것이 다중 퍼셉트론
            
    - 다층 퍼셉트론
        
        
        ![출처 : [https://liveyourit.tistory.com/63](https://liveyourit.tistory.com/63)](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%206.png)
        
        출처 : [https://liveyourit.tistory.com/63](https://liveyourit.tistory.com/63)
        
        - 입력층과 출력층 사이에 하나 이상의 은닉층을 두어 비선형적으로 분리되는 데이터에 대해 학습이 가능한 퍼셉트론
        - 단층 퍼셉트론이 입력층과 출력층이 직접 연결되어 폭표값과 출력값을 직접 비교해 가중치를 조정할 수 있는 것과 달리, 다층 퍼셉트론은 중간에 존재하는 은닉층으로 인해 직접 비교가 불가능
        - 이러한 문제를 해결하기 위해 사용하는 방법이 역전파 알고리즘
        
    - 역전파(backpropagation) 개념
        - 손실함수란 신경망에 훈련데이터를 투입하고 실제 출력과 기대 출력간의 차이라 할 수 있음
        - 손실함수는 결국 가중치와 바이어스 함수로 구성이 되어 있기 때문에 출력에서 생긴 오차를 반대로 입력 쪽으로 전파하면서 가중치와 바이어스를 갱신하게 되면 훈련 데이터에 최적화된 가중치와 바이어스 값들을 얻을 수 있음
        - 역전파랑 용어는 출력부터 반대 방향으로 순차적으로 편미분을 수행해가면서 가중치와 바이어스를 갱신시켜간다는 의미
        
    - 활성함수(activation function)
        - 인공신경망 모형에서 입력신호의 총합을 출력 신호로 변환하는 함수
        - 사용하는 이유는 데이터를 비선형으로 변환하기 위함
        - 과거에는 시그모이드 함수를 주로 사용했으나 양 끝의 정보가 없어지는 기울기 소실문제가 발견되면서 지금은 ReLU 같이 출력의 정보다 계속 유지되는 함수를 많이 적용
        - 종류
            - 계단 함수(step function) : 임계값 기준으로 활성화 또는 비활성화
            - 부호 함수(sign function) : 임계값을 기준으로 양의 부호(+1) 또는 음의 부호(-1)을 출력
            - 시그모이드 함수(sigmoid  function) : 로지스틱 함수라고 하며 특정 임계값을 기준으로 출력값이 급격하게 변하는 계단함수와 달리 완만한 곡선 형태로 0~1사이의 값을 출력
            - tanh 함수(tanh  function) : 확장된 시그모이드 함수로 볼 수 있으며, -1~1사이의 값을 출력
            - ReLU 함수(rectified linear unit function) : 입력값이 0보다 작으면 0, 크면 그대로 출력
            - 소프트맥스 함수(softmax function) : 목표값이 범주인 경우에 사용하며 입력받는 값을 정규화해 0~1 사이의 값으로 출력, 출력값들의 합은 항상 1임
            
            ![출처 : [https://codedragon.tistory.com/6975](https://codedragon.tistory.com/6975)](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%207.png)
            
            출처 : [https://codedragon.tistory.com/6975](https://codedragon.tistory.com/6975)
            
    
    - 경사하강법(gradient method)
        - 인공신경망은 손실함수가 최소화되도록 최적의 가중치와 바이어스를 찾아야함
        - 이 때, 각 지점에서의 손실함수의 값을 낮추는 방안을 제시하는 지표가 기울기
        - 경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동과 기울기를 구하는 과정을 반복하며 기울어진 방향으로 나아감
        - 이를 통해 손실함수의 값을 점차 줄이는 것이 경사하강법
        
    - 기울기 소실문제(vanishing gradient problem)
        - 신경망 모형은 보이는 층(visible layer)과 숨겨진 층(hidden layer)으로 구성됨
        - 보이는 층은 입력층과 출력층으로 구성되어 있고 그 안에서 어떤 계산이 이루어지는지 볼 수 없기 때문에 숨겨진 층 또는 은닉층이라고 함
        - 은닉층이 많은 다층 퍼셉트론에서 은닉층을 많이 거칠수록 전달되는 오차가 크게 줄어들어 학습이 되지 않는 현상이 발생하는데 이를 기울기 소실문제라고 함
        - 기울기가 거의 0으로 소멸되어 버리면 네트워크의 학습은 매누 느려지고, 학습이 다 이루어지지 않은 상태에서 멈추게 되면 신경망은 효과적인 학습을 할 수 없게 됨
        - 시그모이드 함수와 같은 경우 출력값이 1 아래여서 기울기 소실문제가 빠르게 일어남
        - 시그모이드 활성화 함수의 꼬리 부근에서 기울기는 0이 되기 때문에 역전파를 하는 동안 한 층의 노드에서 다른 층의 노드로 흘러야 할 신호가 거의 없게 됨
        - 이러한 현상은 다층일 때 더욱 악화됨
        - 그 결과로 입력층에 큰 변화가 있더라도 출력층에 크게 변화가 없는 현상이 발생함
    
    - 인공신경망의 은닉층 수, 은닉 노드 수를 정할 때 고려해야할 사항
        - 다층 신경망은 단층 신경망에 비해 훈련이 어려움
        - 시그모이드 활성함수를 가지는 2개 층의 네트워크는 임의의 의사결정 경계를 모형화할 수 있음
        - 노드가 많은수록 복잡성을 잡아내기 쉽지만, 과적합의 가능성도 높아짐
        - 은닉층 노드가 너무 적으면 복잡한 의사결정 경계를 만들 수 없음
        - 출력층 노드의 수는 출력 범주의 수로 결정, 입력의 수는 입력 차원의 수로 결정
        
    - 인공신경망의 장단점
        - 장점
            - 변수의 수가 많거나 입력, 출력변수간의 복잡한 비선형 관계에 유용
            - 잡음에 대해서도 민감하게 반응하지 않음
            - 입력변수와 결과변수가 연속형이나 이산형인 경우 모두 처리 가능
        - 단점
            - 결과에 대한 해석이 쉽지 않음
            - 최적의 모형을 도출하는 것이 상대적으로 어려움
            - 데이터 정규화를 하지 않으면 지역해(local minimum)에 빠질 위험이 존재
            - 모형이 복잡하면 훈련 과정에 시간이 많이 소요됨
            
- 예제 : iris data에 대한 신경망 모형 분석
    
    ```r
    colnames(iris) = tolower(colnames(iris))
    install.packages("nnet")
    library(nnet)
    data = iris
    Scale = data.frame(lapply(data[,1:4], function(x) scale(x)))
    Scale$species = data$species
    index = sample(1:nrow(Scale), round(0.75*nrow(Scale)), replace=FALSE)
    train = Scale[index,]
    test = Scale[-index,]
    model = nnet(species~.,size=2,decay=5e-04,data=train)
    > summary(model)
    a 4-2-3 network with 19 weights
    options were - softmax modelling  decay=5e-04
     b->h1 i1->h1 i2->h1 i3->h1 i4->h1 
    -11.89   2.87  -1.76   3.32  12.27 
     b->h2 i1->h2 i2->h2 i3->h2 i4->h2 
      2.74   1.85  -2.09   2.50   2.11 
     b->o1 h1->o1 h2->o1 
      6.16  -2.01  -8.72 
     b->o2 h1->o2 h2->o2 
     -1.97 -11.24   9.18 
     b->o3 h1->o3 h2->o3 
     -4.21  13.21  -0.44
    
    predict.model = predict(model,test[,1:4],type="class")
    predict.model
    actual = test$species
    confusion.matrix = table(actual, predict.model)
    
    > confusion.matrix
                predict.model
    actual       setosa versicolor virginica
      setosa         14          0         0
      versicolor      0          9         1
      virginica       0          4        10
    
    > sum(predict.model==actual)/NROW(predict.model)
    [1] 0.8684211
    ```
    
    - summary함수를 이용해 결과 확인 : 출력,은닉,입력 노드의 수와 가중치의 수를 보여준 후, 입력>은닉 노드로 가는 가중치의 값과 은닉>출력노드로 가는 가중치의 값을 보여줌
    - 가중치의 수 = (입력노드+1)*은닉노드+(은닉노드+1)*출력노드
    - 혼동행렬을 만들고 정확도를 계산한 결과, 약 87%의 값을 나타냄
    

## 3) 의사결정나무 모형(Decision tree)

- 의사결정나무 개념
    - 의사결정 규칙을 구조로 나타내어 전체 자료를 몇 개의 소집단으로 분류하거나 예측을 수행하는 분석 방법
    - 의사결정나무의 구조
        
        ![출처 : [https://velog.io/@noooooh_042/의사결정나무](https://velog.io/@noooooh_042/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4)](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%208.png)
        
        출처 : [https://velog.io/@noooooh_042/의사결정나무](https://velog.io/@noooooh_042/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4)
        
        - 뿌리 마디(root node) : 시작되는 마디로 전체 자료를 포함
        - 자식 마디(child node) : 하나의 마디로부터 분리되어 나간 2개 이상으 ㅣ마디들
        - 부모 마디(parent node) : 주어진 마디의 상위 마디
        - 최종 마디(terminal node) : 자식마디가 없는 마디
        - 중간 마디(internal node) : 부모 마디와 자식 마디가 모두 있는 마디
        - 가지(branch) : 뿌리마디로부터 최종마디까지 연결된 마디들
        - 깊이(depth) : 뿌리마디부터 최종마디까지의 중간 마디들의 수
    - 매 단계마다 분류변수와 분류기준 값의 선택이 중요
        - 하위 노드에서 노드 내에서는 동질성이 노드간에는 이질성이 가장 커지도록 선택
    - 나무모형의 크기는 과대적합 또는 과소적합되지 않도록 합리적기준에 의해 적당하게 조절되어야 함
    
- 의사결정나무의 형성
    - 의사결정나무 생성 : 분석의 목적과 자료구조에 따라서 적절한 분리 기준과 정지 규칙을 지정하여 의사결정나무를 생성
    - 가지치기 : 부적절한 가지는 제거
    - 타당성 평가 : 이익도표나 위험도표 또는 검증용 데이터에 의한 교차 타당성 등을 이용해 평가
    - 분류 및 예측 : 의사결정나무를 해석하고 예측모형 설정
    
- 의사결정나무의 분리 기준
    - 순수도란 목표변수의 특정 범주에 개체들이 포함되어 있는 정도를 의미
    - 결국 분리 기준은 부모 마디에 비해 자식 마디에서 순수도가 증가하는 정도를 수치화한 것
    - 부모 마디의 순수도보다 자식 마디의 순수도가 증가하는 방향으로 트리구조를 형성
    
- 가지치기와 정지규칙
    - 가지치기
        - 모든 터미널 노드의 불순도가 0인상태를 full tree라고 함
        - 나무의 크기는 곧 모형의 복잡도를 의미
        - 복잡도가 높아질수록 과적합의 위험이 존재
        - 따라서 적합한 수준에서 터미널 노드를 결합해주는 것이 필요하며, 이를 가지치기라고 함
    - 정지규칙
        - 더 이상 트리의 분리가 일어나지 않게 하는 규칙
        - 정지규칙을 적용하지 않으면 각 끝마디가 full tree까지 성장하므로 과적합이 발생할 수 있음
        
- 의사결정나무 모형의 분류
    - 분류나무(classification tree)
        - 목표변수가 이산형
        - 분류 기준값으로 카이제곱 통계량의 p값, 지니지수, 엔트로피지수 등이 사용됨
        - 카이제곱 p값은 값이 작을수록, 지니 지수와 엔트로피 지수는 값이 클수록 자식 노드 내에서 이질성이 큼을 의미
        - 불순도 측도
            
            
            | 구분 | 공식 | 설명 |
            | --- | --- | --- |
            | 카이제곱 통계량 | ⁍ | 데이터의 분포와 사용자가 선택한 기대 또는 가정된 분포 사이의 차이를 나타내는 측정값 |
            | 지니 지수 | ⁍ | 지니 지수의 값이 클수록 이질적이며 순수도가 낮다고 볼 수 있음 |
            | 엔트로피 지수 | ⁍ | 엔트로피 지수의 값이 클수록 순수도가 낮다고 볼 수 있음 |
    - 회귀나무(regression tree)
        - 목표변수가 연속형
        - 분류 기준값으로 F통계량의 p값, 분산의 감소량 등이 사용됨
        - F통계량은 값이 클수록 자식노드 간이 이질적임을 의미하므로 이 값이 커지는(p값은 작아지는) 방향으로 가지 분할을 수행
        - 분산의 감소량도 이 값이 최대화되는 방향으로 가지 분할을 수행
        
- 의사결정나무 알고리즘과 분류기준
    - 이산형 목표변수
        
        
        | 알고리즘 | 분류기준 | 설명 |
        | --- | --- | --- |
        | CHAID | 카이제곱 통계량 | p값이 작아지는 방향으로 가지 분할 |
        | CART | 지니 | 지니 지수가 작아지는 방향으로 가지 분할 |
        | C4.5, C5.0 | 엔트로피 | 엔트로피 지수가 작아지는 방향으로 가지 분할 |
    - 연속형 목표변수
        
        
        | 알고리즘 | 분류기준 | 설명 |
        | --- | --- | --- |
        | CHAID | ANOVA F-통계량 | p값이 작아지는 방향으로 가지 분할 |
        | CART | 분산감소량 | 분산감소량이 커지는 방향으로 가지 분할 |

- 의사결정나무의 장단점
    - 장점
        - 구조가 단순하여 해석이 용이
        - 유용한 입력변수의 파악과 예측변수 간의 상호작용 및 비선형성을 고려하여 분석이 가능
        - 선형성, 정규성, 등분산성 등의 수학적 가정이 불필요한 비모수적 모형
        - 계산 비용이 낮아 대규모의 데이터셋에서도 비교적 빠르게 연산이 가능
        - 수치형/범주형 변수를 모두 사용할 수 있음
    - 단점
        - 분류 기준값의 경계선 부근의 자료값에 대해서 오차가 큼
        - 로지스틱회귀와 같이 각 예측변수의 효과를 파악하기 어려움
        - 새로운 자료에 대한 예측이 불안정할 수 있음

- rpart 패키지를 이용한 iris 데이터 의사결정나무 분석
    
    ```r
    > colnames(iris) = tolower(colnames(iris))
    > library('rpart')
    > k = rpart(species~.,data=iris)
    > k
    n= 150 
    
    node), split, n, loss, yval, (yprob)
          * denotes terminal node
    
    1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)  
      2) petal.length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
      3) petal.length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)  
        6) petal.width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
        7) petal.width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
    > plot(k,compress=T,margin=.3)
    > text(k,cex=1.0)
    ```
    
    ![Untitled](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%209.png)
    
    - 결과의 들여쓰기는 가지가 갈려지는 모양, *는 잎사귀 노드, 각 노드에서 괄호안에 표시된 숫자는 species별 비율을 의미
    - 결과의 2) 부분 해석 : 뿌리노드에서 좌측가지 밑에 위치하며 <2.45. 기준을 만족하는 경우 setosa이고 50개의 데이터가 존재
    - rpart.plot을 활용한 시각화
        
        ```r
        install.packages('rpart.plot')
        library('rpart.plot')
        prp(k,type=4,extra=2,digits=3)
        ```
        
        ![Untitled](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%2010.png)
        
    - 예측과 검증
        
        ```r
        > head(predict(k,newdata=iris,type='class'))
             1      2      3      4      5      6 
        setosa setosa setosa setosa setosa setosa 
        Levels: setosa versicolor virginica
        > printcp(k)
        
        Classification tree:
        rpart(formula = species ~ ., data = iris)
        
        Variables actually used in tree construction:
        [1] petal.length petal.width 
        
        Root node error: 100/150 = 0.66667
        
        n= 150 
        
            CP nsplit rel error xerror     xstd
        1 0.50      0      1.00   1.25 0.045644
        2 0.44      1      0.50   0.84 0.060795
        3 0.01      2      0.06   0.10 0.030551
        > plotcp(k)
        > install.packages('caret')
        > library(caret)
        > install.packages('e1071')
        > library(e1071)
        > rpartpred = predict(k,iris,type='class')
        > confusionMatrix(rpartpred,iris$species)
        Confusion Matrix and Statistics
        
                    Reference
        Prediction   setosa versicolor virginica
          setosa         50          0         0
          versicolor      0         49         5
          virginica       0          1        45
        
        Overall Statistics
                                                 
                       Accuracy : 0.96           
                         95% CI : (0.915, 0.9852)
            No Information Rate : 0.3333         
            P-Value [Acc > NIR] : < 2.2e-16      
                                                 
                          Kappa : 0.94           
                                                 
         Mcnemar's Test P-Value : NA             
        
        Statistics by Class:
        
                             Class: setosa Class: versicolor Class: virginica
        Sensitivity                 1.0000            0.9800           0.9000
        Specificity                 1.0000            0.9500           0.9900
        Pos Pred Value              1.0000            0.9074           0.9783
        Neg Pred Value              1.0000            0.9896           0.9519
        Prevalence                  0.3333            0.3333           0.3333
        Detection Rate              0.3333            0.3267           0.3000
        Detection Prevalence        0.3333            0.3600           0.3067
        Balanced Accuracy           1.0000            0.9650           0.9450
        ```
        
        ![Untitled](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%2011.png)
        
        - 96%의 정확도를 확인 가능

## 4) 앙상블 모형

- 여러 개의 분류모형에 의한 결과를 종합하여 분류의 정확도를 높이는 방법
- 적절한 표본추출법으로 데이터에서 여러 개의 훈련용 데이터 집합을 만들어 각각의 데이터 집합에서 하나의 분류기를 만들어 앙상블하는 방법

- 배깅(bagging)
    - bootstrap aggregating의 준말
    - 원 데이터 집합으로부터 크기가 같은 표본을 여러 번 단순 임의 복원 추출하여 각 표본에 대해 분류기를 생성한 후 그 결과를 앙상블하는 방법
    - 복원추출이기 때문에 같은 데이터가 한 표본에 여러 번 추출될 수도 있고, 추출되지 않을 수도 있음
    - 배깅은 분산을 감소시키기 위해, 훈련데이터에서 많은 샘플링을 하기 때문에 편향이 작고 분산이 높은 모델이 사용하면 효과적
    - 부스트랩 방법
        - 표본 데이터를 하나의 모집단으로 취급해서 그것으로부터 더 작은 표본들을 복원추출로 수집한 후, 각 부스트랩 샘플로부터 통계량을 계싼하고 다수의 샘플을 구성함으로써 표집분포를 추정하는 방법
        - 적은 데이터라도 정규분포를 형성시켜 모집단의 평균을 추정할 수 있음
        - 한 번도 추출되지 않은 샘플들은 검증 데이터로 활용할 수 있으며, 이런 training observation을 out-of-bag observation이라고 부름
    - iris data를 활용한 배깅(출처:[https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=kimdansay&logNo=221393641258](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=kimdansay&logNo=221393641258))
        
        ```r
        install.packages('adabag')
        library(adabag)
        library(rpart)
        data(iris)
        #test 데이터 생성 : 배깅은 이 test data를 모집단으로 생각하고 평균예측모형을 구하기 때문에 분산을 줄이고 예측력을 향상시킬 수 있다.
        set.seed(1)
        train <- c(sample(1:50,25),sample(51:100,25),sample(101:150,25))
        
        #bagging 모델링 적용
        iris.bagging<-bagging(Species~.,data=iris[train,], mfinal=10,control=rpart.control(maxdepth=1))
        iris.bagging
        #mfinal : tree 반복생성 횟수, control은 tree분석의 "rpart.control" 적용
        
        #iris.bagging 결과 : 의사결정 나무 결과 10개, vote 결과, 최종 결정 결과, bootstrap sample 10개, 변수의 중요도
        #tree를 호출하면 각기 다른 tree가 생성됨을 확인 할 수 있다.
        iris.bagging$tree[[1]]
        iris.bagging$tree[[2]]
        
        #test data에서 랜덤 복원추출(Random Sampling) 한 bootstrap 자료 확인 _ 복원추출이라서 하나의 값이 여러번 뽑힌 것을 확인 할 수 있다.
        sample1<-iris.bagging$samples[,1] 
        sample2<-iris.bagging$samples[,2]
        table(sample1)
        table(sample2)
        
        #voting에 의한 최종 결정 결과 확인
        iris.bagging$class           
        
        #변수의 중요도 출력
        iris.bagging$importance      
        barplot(iris.bagging$importance[order(iris.bagging$importance, decreasing=TRUE)],ylim=c(0,100), main="Variables Relative Importance", col="blue")
        
        #배깅 모형의 정확도 계산하기
        Table <- table(iris.bagging$class, iris$Species[train], dnn=c("Predicted Class", "Observed Class"))
        accuracy <- sum(diag(Table))/sum(Table)
        ```
        

- 부스팅(boosting)
    - 배깅과 유사하나, 샘플링 과정에서 각 자료에 동일한 확률을 부여하는 것이 아니라, 분류가 잘못된 데이터에 더 큰 가중을 주어 표본을 추출
    - 약한 분류기의 결과를 바탕으로 가중치를 부여하여 다음 분류기로 만드는 과정을 반복해 강한 분류기를 만듬
    - iris 자료를 활용한 부스팅(출처 : [https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=kimdansay&logNo=221393641258](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=kimdansay&logNo=221393641258))
        
        ```r
        #iris 데이터 - 부스팅(Adaboost)기법 적용한 분류모델
        
        library(adabag)
        set.seed(1)
        train <- c(sample(1:50,25),sample(51:100,25), sample(101:150,25))
        iris.adaboost <- boosting(Species~.,data=iris[train,],
        mfinal=10,control=rpart.control(maxdepth=1))
        iris.adaboost
        
        #의사결정나무 10개, weights 10개, vote로 인한 최종결정(class), 변수의 중요도(importance)
        iris.adaboost$importance
        
        #bagging의 결과와 약간 다르다.
        barplot(iris.adaboost$importance[order(iris.adaboost$importance,decreasing=TRUE)],ylim=c(0,100), main="Variable relative importance")
        
        #부스팅 모형의 정확도 계산하기
        Table <- table(iris.adaboost$class, iris$Species[train], dnn=c("Predicted Class", "Observed Class"))
        accuracy <- sum(diag(Table))/sum(Table) 
        
        #배깅과 정확도 일치한다.
        ```
        

- 랜덤 포레스트(random forest)
    - 배깅에 랜덤과정을 추가한 방법
    - 원 자료로부터 부스트랩 샘플을 추출하고, 각 부스트랩 샘플에 대해 트리를 형성해 나가는 과정은 배깅과 유사
    - 각 노드마다 모든 예측변수 안에서 최적의 분할을 선택하는 방법 대신 예측변수들을 임의로 추출하고, 추출된 변수 내에서 최적의 분할을 만들어 나가는 방법을 사용
    - 새로운 자료에 대한 예측은 분류는 다수결, 회귀는 평균을 취하는 방법을 사용하며 이는 다른 앙상블 모형과 동일
    - 각 가지를 나누는 변수를 선택할 때 전체 변수를 매번 모두 고려하는 대신 변수의 일부를 임의로 선택하는 특징을 가짐
    - 변수의 중요도 평가
        - randomFrest()는 설명 변수의 중요도를 평가하는데 사용할 수 있음
        - 이 방법은 각 변수들이 지니 또는 정확도에 얼마만큼 기여하는지를 통해 변수의 중요도를 판별
        - 변수의 중요도를 알아보려면 함수에서 importance=TRUE를 지정
        - importance(), varImpPlot()를 사용해 결과를 출력
        - 결과를 통해 각 정확도와 지니 측면에서 중요도를 확인 가능
    - 랜덤 포레스트의 파라미터
        - 랜덤 포레스트에는 트리갯수(ntree), 각 노드에서 가지를 칠 때 고려할 변수의 개수(mtry) 등의 파라미터가 있음
    - 결과에서 OOB(out of bag)의 의미
        - 모델 훈련에 사용되지 않은 데이터를 사용한 에러 추정치가 출력
    - ploidy 자료 랜덤포레스트 분석
        
        ```r
        install.packages('randomForest')
        library(randomForest)
        library(rpart)
        data(stagec)
        stagec3 <- stagec[complete.cases(stagec),]
        set.seed(1234)
        ind <- sample(2, nrow(stagec3), replace = TRUE, prob=c(0.7, 0.3))
        trainData<-stagec3[ind==1, ]
        testData<-stagec3[ind==2, ]
        rf <- randomForest(ploidy~., data=trainData, ntree=100, proximity=TRUE)
        table(predict(rf), trainData$ploidy)
        print(rf)
        plot(rf)
        importance(rf)
        varImpPlot(rf)
        
        rf.pred <- prdict(rf, newdata=testData)
        table(rf.pred, testData$ploidy)
        plot(margin(rf))
        
        set.seed(1234)
        cf <- cforest(ploidy ~ ., data=trainData)
        cf.pred <- predict(cf, newdata=testData, OOB=TRUE, type="response")
        ```
        

## 5) 서포트 벡터 머신(Support Vector Machine)

- 서포트 벡터 머신의 개념
    - 고차원 또는 무한 차원의 공간에서 초평면을 찾아 이를 이용해 분류와 회귀를 수행
    - 패턴인식, 자료분석을 위한 지도학습 모델이며 2개의 범주를 분류하는 이진 분류기
    - 주로 분류와 회귀분석을 위해 사용
    - SVM 알고리즘은 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어느 카테고리에 속할 것인지 판단하는 비확률적 이진 선형 분류 모델을 만듬
    - 기본적인 원리는 두 그룹에서 각각의 데이터 간 거리를 측정하여 두 개의 데이터 사이의 중심을 구한 후에 그 가운데에서 최적의 초평면(optimal hyper plane)을 구함으로써 그룹을 나눔
    - 여기서 직선으로 나눌 수 있다면 선형 분류 모델을 적용하고, 불가능한 경우 비선형 분류 모델을 사용하게 됨
    - 자료를 군집 별로 가장 잘 분리하는 초평면으로부터 가장 가까운 훈련용 자료까지의 거리를 마진(margin)이라 하고, 마진이 가장 큰 초평면을 분류기로 사용할 때, 새로운 자료에 대한 오분류가 가장 낮아짐
    - 선형 분류 뿐만 아니라 커널 트릭이라 불리는 다차원 공간상으로의 매핑 기법을 사용해 비선형 분류도 수행 가능
    
- 서포트 벡터 머신의 장단점
    - 장점
        - 비선형 데이터도 커널 트릭을 이용해 분류가 가능
        - 인공신경망보다 과적합의 위험이 적음
        - 노이즈의 영향이 적음
    - 단점
        - 파라미터와 커널 선택에 민감
        - 데이터셋의 크기가 클 경우 모델링에 많은 시간이 소요됨
    
- iris 데이터 분류 및 예측
    
    ```r
    library(e1071)
    s = sample(150,100)
    col = c('Petal.Length','Petal.Width','Species')
    iris_train = iris[s,col]
    iris_test = iris[-s,col]
    iris_svm = svm(Species~.,data=iris_train,cost=1,kernel='linear')
    plot(iris_svm,iris_train[,col])
    p = predict(iris_svm,iris_test[,col],type='class')
    plot(p)
    table(p,iris_test[,3])
    
    p            setosa versicolor virginica
      setosa         18          0         0
      versicolor      0         17         1
      virginica       0          1        13
    ```
    
    ![Untitled](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%2012.png)
    
    ![Untitled](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%2013.png)
    

## 6) 나이브 베이즈 분류모형

- 나이브 베이즈 분류모형의 개념
    - 베이즈정리에 기반한 방법으로, 사후확률의 계산 시 조건부 독립을 가정하여 계산을 단순화함
    - 사후확률이 큰 집단으로 새로운 데이터를 분류하게 됨
    - 조건부 독립의 가정이 비현실적인 측면이 있으나 계산이 간편하여 널리 이용되고 있음
    - 사후확률은 사전확률을 통해 예측할 수 있다라는 의미에 근거하여 분류 모형을 예측

- 나이브 베이즈 분류의 장단점
    - 장점
        - 지도학습 환경에서 매우 효율적으로 훈련할 수 있으며, 분류에 필요한 파라미터를 추정하기 위한 훈련 데이터가 매우 적어도 사용할 수 있음
        - 분류가 multi class에서 쉽고 빠르게 예측이 가능
    - 단점
        - 훈련 데이터에는 없고, 검증 데이터에 있는 범주에서는 확률이 0으로 나타나 정상적인 예측이 불가능한 zero frequency가 됨
        - 서로 확률적으로 독립이라는 가정이 위반되는 경우에 오류가 발생
        
- 나이브 베이즈 분류 모형
    
    ```r
    > data(iris)
    > colnames(iris) = tolower(colnames(iris))
    > library(e1071)
    > m = naiveBayes(species~.,data=iris)
    > m
    
    Naive Bayes Classifier for Discrete Predictors
    
    Call:
    naiveBayes.default(x = X, y = Y, laplace = laplace)
    
    A-priori probabilities:
    Y
        setosa versicolor  virginica 
     0.3333333  0.3333333  0.3333333 
    
    Conditional probabilities:
                sepal.length
    Y             [,1]      [,2]
      setosa     5.006 0.3524897
      versicolor 5.936 0.5161711
      virginica  6.588 0.6358796
    
                sepal.width
    Y             [,1]      [,2]
      setosa     3.428 0.3790644
      versicolor 2.770 0.3137983
      virginica  2.974 0.3224966
    
                petal.length
    Y             [,1]      [,2]
      setosa     1.462 0.1736640
      versicolor 4.260 0.4699110
      virginica  5.552 0.5518947
    
                petal.width
    Y             [,1]      [,2]
      setosa     0.246 0.1053856
      versicolor 1.326 0.1977527
      virginica  2.026 0.2746501
    
    > table(predict(m, iris[,-5]),iris[,5],dnn=list('predicted','actual'))
                actual
    predicted    setosa versicolor virginica
      setosa         50          0         0
      versicolor      0         47         3
      virginica       0          3        47
    ```
    

## 7) KNN

- KNN 개념
    - 새로운 데이터에 대해 이와 가장 유사한 k개의 과거 자료의 결과를 이용해 다수결로 분류
    - 과거 자료를 이용하여 미리 분류모형을 수립하는 것이 아니라, 과거 데이터를 저장만 해두고 필요시 새로운 데이터를 비교하여 분류하는 방식
    - 반응변수가 범주형인 경우에는 분류로, 연속형인 경우에는 회귀의 목적으로 사용할 수 있음
    
- KNN 알고리즘의 이해
    
    ![출처 : [https://bkshin.tistory.com/entry/머신러닝-6-K-최근접이웃KNN](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-6-K-%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83KNN)](3%203%203%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%203bde6da630c8422ca22cde4129e536e9/Untitled%2014.png)
    
    출처 : [https://bkshin.tistory.com/entry/머신러닝-6-K-최근접이웃KNN](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-6-K-%EC%B5%9C%EA%B7%BC%EC%A0%91%EC%9D%B4%EC%9B%83KNN)
    
    - 그림과 같이 k=3이면 class B로 k=6이면 class A로 분류
    - 학습이라고 할 만한 절차가 없음
    - 따라서 lazy model, instsnce based learning(사례기반학습)이라함